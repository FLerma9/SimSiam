{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa0c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from simsiam.model_factory import SimSiam\n",
    "import neptune.new as neptune\n",
    "import math\n",
    "import time\n",
    "from torchvision.models import alexnet, vgg13, resnet34, squeezenet1_1, mobilenet_v2, mobilenet_v3_small\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e3e5a",
   "metadata": {},
   "source": [
    "Hiper-parámetros que tiene que definir el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d0371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'mobilenet_v2' #Opciones: resnet34, mobilenet_v2, alexnet, vgg13 y squeezenet1_1\n",
    "train_path = 'ImageWoof/train' #Path del dataset con las imágenes de train\n",
    "val_path = 'ImageWoof/val'  #Path del dataset con las imágenes de val\n",
    "model_path = 'Classifier_Checkpoints/ImageWoof/Linear_Classifier/Mobilenet/100epochs/' #Path donde se querrá guardar el modelo\n",
    "#Path donde se ha guardado el modelo preentrenado con SimSiam\n",
    "pretrained_path = 'simsiam/Unsupervised_Training_Checkpoints/ImageWoof/Mobilenet/checkpoint_100.pth'\n",
    "batch_size = 256\n",
    "epochs = 90 #número de épocas\n",
    "lr = 0.4 #base_lr\n",
    "weight_decay = 0.0\n",
    "momentum = 0.9\n",
    "gpu = 0 #gpu donde ejecutar\n",
    "nombre = 'MobilenetV2ImageWoofSimSiamLinear100epochsPre'#nombre del experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae9ecb",
   "metadata": {},
   "source": [
    "Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d49d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab65c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7c83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d80671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \"\"\"\n",
    "    Switch to eval mode:\n",
    "    Under the protocol of linear classification on frozen features/models,\n",
    "    it is not legitimate to change any part of the pre-trained model.\n",
    "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
    "    no gradient), which are part of the model parameters too.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images = images.cuda(0, non_blocking=True)\n",
    "        target = target.cuda(0, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "    \n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79fb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.cuda(0, non_blocking=True)\n",
    "            target = target.cuda(0, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85eea5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, n_epochs):\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = cur_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad14f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(backbone_name):\n",
    "        return {'alexnet': alexnet(num_classes=10),\n",
    "                'mobilenet_v2': mobilenet_v2(num_classes=10),\n",
    "                'squeezenet1_1': squeezenet1_1(num_classes=10),\n",
    "                'resnet34': resnet34(num_classes=10),\n",
    "                'vgg13': vgg13(num_classes=10)}[backbone_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23188f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomResizedCrop(224),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [transforms.Resize((256,256)),\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_set = datasets.ImageFolder(train_path, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_set = datasets.ImageFolder(val_path, transform=transform_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e18a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_backbone(arch)\n",
    "#congelamos todos los parametros menos la fc clasificadora que entrenaremos\n",
    "classifier = 'classifier'\n",
    "if 'resnet' in arch:\n",
    "    classifier = 'fc'\n",
    "for name, param in model.named_parameters():\n",
    "    if classifier not in name:\n",
    "        param.requires_grad = False     \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf7985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['classifier.1.weight', 'classifier.1.bias'], unexpected_keys=[])\n",
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# inicializamos la fc clasificadora\n",
    "if arch == 'resnet34':\n",
    "    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.fc.bias.data.zero_()\n",
    "elif arch == 'alexnet':\n",
    "    model.classifier[1].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[4].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[6].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[1].bias.data.zero_()\n",
    "    model.classifier[4].bias.data.zero_()\n",
    "    model.classifier[6].bias.data.zero_()\n",
    "elif arch== 'vgg13':\n",
    "    model.classifier[0].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[3].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[6].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[0].bias.data.zero_()\n",
    "    model.classifier[3].bias.data.zero_()\n",
    "    model.classifier[6].bias.data.zero_()\n",
    "else:\n",
    "    model.classifier[1].weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.classifier[1].bias.data.zero_()\n",
    "\n",
    "# cargamos el modelo autosupervisado preentrenado\n",
    "checkpoint = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "#obtenemos el diccionario con los parámetros preentrenados\n",
    "state_dict = checkpoint.state_dict()\n",
    "\n",
    "for k in list(state_dict.keys()):\n",
    "    # Si el parámetro pertenece al backbone y no a la capa clasificadora\n",
    "    if k.startswith('encoder') and clasifier not in k:\n",
    "        #La renombramos\n",
    "        state_dict[k[len(\"encoder.\"):]] = state_dict[k]\n",
    "    # Borramos los parámetros que no son del backbone o los que hemos renombrado\n",
    "    del state_dict[k]\n",
    "#cargamos los parámetros del preentreno al linear classifier        \n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "#nos aseguramos que se hayan cargado todos menos los de la ultima FC\n",
    "if arch == 'resnet34':\n",
    "    assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
    "elif arch == 'alexnet':\n",
    "    assert set(msg.missing_keys) == {\"classifier.1.weight\",\"classifier.4.weight\", \"classifier.6.weight\", \"classifier.1.bias\", 'classifier.4.bias', 'classifier.6.bias'}\n",
    "elif 'vgg' in arch:\n",
    "    assert set(msg.missing_keys) == {\"classifier.0.weight\",\"classifier.3.weight\", \"classifier.6.weight\", \"classifier.0.bias\", 'classifier.3.bias', 'classifier.6.bias'}\n",
    "else:\n",
    "    assert set(msg.missing_keys) == {\"classifier.1.weight\", \"classifier.1.bias\"}\n",
    "\n",
    "print(model)\n",
    "#lo enviamos a la GPU\n",
    "if gpu is not None:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model = model.cuda(gpu)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c75d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = lr * batch_size / 256\n",
    "criterion = nn.CrossEntropyLoss().cuda(0)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "if arch == 'resnet34':\n",
    "    assert len(parameters) == 2\n",
    "elif arch == 'alexnet':\n",
    "    assert len(parameters) == 6 \n",
    "elif 'vgg' in arch:\n",
    "    assert len(parameters) == 6\n",
    "else:\n",
    "    assert len(parameters) == 2\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, init_lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce5f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/franciscolm6/TFGInformatica/e/TFGIN-192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\anaconda3\\lib\\site-packages\\neptune\\new\\internal\\utils\\git.py:37: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch: [0][ 0/36]\tTime 11.322 (11.322)\tData  7.383 ( 7.383)\tLoss 2.3193e+00 (2.3193e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  51.56 ( 51.56)\n",
      "Epoch: [0][10/36]\tTime  0.150 ( 1.192)\tData  0.000 ( 0.694)\tLoss 1.5225e+01 (1.1510e+01)\tAcc@1   8.98 ( 14.81)\tAcc@5  56.25 ( 55.97)\n",
      "Epoch: [0][20/36]\tTime  0.154 ( 0.817)\tData  0.000 ( 0.479)\tLoss 2.7640e+01 (1.6055e+01)\tAcc@1  14.84 ( 14.92)\tAcc@5  58.59 ( 57.74)\n",
      "Epoch: [0][30/36]\tTime  0.155 ( 0.658)\tData  0.000 ( 0.379)\tLoss 1.6924e+01 (1.7126e+01)\tAcc@1  20.70 ( 15.85)\tAcc@5  64.06 ( 58.87)\n",
      " * Acc@1 16.722 Acc@5 65.411\n",
      "EPOCH 1 ENDED: LOSS train 16.77627211679052 LOSS val 12.76944652905929 Train Acc 15.822714805603027 Val Acc 16.721811294555664\n",
      "Epoch: [1][ 0/36]\tTime  7.641 ( 7.641)\tData  7.152 ( 7.152)\tLoss 1.2224e+01 (1.2224e+01)\tAcc@1  20.31 ( 20.31)\tAcc@5  61.33 ( 61.33)\n",
      "Epoch: [1][10/36]\tTime  0.152 ( 0.965)\tData  0.000 ( 0.775)\tLoss 8.9026e+00 (8.9711e+00)\tAcc@1  21.48 ( 22.09)\tAcc@5  68.36 ( 66.94)\n",
      "Epoch: [1][20/36]\tTime  0.856 ( 0.712)\tData  0.706 ( 0.539)\tLoss 1.9235e+01 (1.1793e+01)\tAcc@1  20.31 ( 19.40)\tAcc@5  67.58 ( 63.82)\n",
      "Epoch: [1][30/36]\tTime  0.151 ( 0.580)\tData  0.001 ( 0.413)\tLoss 1.1504e+01 (1.3334e+01)\tAcc@1  17.19 ( 19.04)\tAcc@5  60.94 ( 64.16)\n",
      " * Acc@1 14.431 Acc@5 58.768\n",
      "EPOCH 2 ENDED: LOSS train 13.839197872933259 LOSS val 19.346070101255677 Train Acc 18.692520141601562 Val Acc 14.43115234375\n",
      "Epoch: [2][ 0/36]\tTime  7.370 ( 7.370)\tData  6.902 ( 6.902)\tLoss 2.0729e+01 (2.0729e+01)\tAcc@1  12.50 ( 12.50)\tAcc@5  56.25 ( 56.25)\n",
      "Epoch: [2][10/36]\tTime  0.151 ( 0.934)\tData  0.000 ( 0.750)\tLoss 1.6842e+01 (1.6380e+01)\tAcc@1  21.09 ( 18.32)\tAcc@5  72.66 ( 62.86)\n",
      "Epoch: [2][20/36]\tTime  0.355 ( 0.653)\tData  0.205 ( 0.485)\tLoss 2.3717e+01 (1.6554e+01)\tAcc@1  16.41 ( 18.23)\tAcc@5  57.42 ( 63.45)\n",
      "Epoch: [2][30/36]\tTime  0.154 ( 0.565)\tData  0.000 ( 0.403)\tLoss 2.3380e+01 (1.9092e+01)\tAcc@1  20.31 ( 17.99)\tAcc@5  61.72 ( 62.55)\n",
      " * Acc@1 19.903 Acc@5 67.091\n",
      "EPOCH 3 ENDED: LOSS train 19.040102229448568 LOSS val 16.812653069703686 Train Acc 18.105262756347656 Val Acc 19.903282165527344\n",
      "Epoch: [3][ 0/36]\tTime  7.313 ( 7.313)\tData  6.905 ( 6.905)\tLoss 1.7320e+01 (1.7320e+01)\tAcc@1  19.92 ( 19.92)\tAcc@5  62.89 ( 62.89)\n",
      "Epoch: [3][10/36]\tTime  0.151 ( 0.925)\tData  0.001 ( 0.751)\tLoss 1.8939e+01 (1.8281e+01)\tAcc@1  17.97 ( 17.29)\tAcc@5  63.67 ( 64.99)\n",
      "Epoch: [3][20/36]\tTime  1.409 ( 0.704)\tData  1.253 ( 0.540)\tLoss 1.5938e+01 (1.8299e+01)\tAcc@1  23.83 ( 18.34)\tAcc@5  71.09 ( 65.53)\n",
      "Epoch: [3][30/36]\tTime  0.159 ( 0.580)\tData  0.000 ( 0.416)\tLoss 1.3948e+01 (1.7100e+01)\tAcc@1  15.62 ( 18.66)\tAcc@5  67.19 ( 65.55)\n",
      " * Acc@1 19.598 Acc@5 66.964\n",
      "EPOCH 4 ENDED: LOSS train 16.614222978079418 LOSS val 13.254593020512209 Train Acc 18.836565017700195 Val Acc 19.597862243652344\n",
      "Epoch: [4][ 0/36]\tTime  7.552 ( 7.552)\tData  7.163 ( 7.163)\tLoss 1.2192e+01 (1.2192e+01)\tAcc@1  21.88 ( 21.88)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [4][10/36]\tTime  0.153 ( 0.942)\tData  0.000 ( 0.769)\tLoss 1.6229e+01 (1.5122e+01)\tAcc@1  20.70 ( 19.74)\tAcc@5  65.23 ( 63.88)\n",
      "Epoch: [4][20/36]\tTime  0.836 ( 0.691)\tData  0.684 ( 0.528)\tLoss 1.8257e+01 (1.4736e+01)\tAcc@1   9.77 ( 19.72)\tAcc@5  64.84 ( 65.62)\n",
      "Epoch: [4][30/36]\tTime  0.152 ( 0.568)\tData  0.000 ( 0.409)\tLoss 2.4286e+01 (1.6239e+01)\tAcc@1  21.48 ( 19.38)\tAcc@5  59.77 ( 63.86)\n",
      " * Acc@1 20.565 Acc@5 63.451\n",
      "EPOCH 5 ENDED: LOSS train 16.730753044593367 LOSS val 23.023048821524768 Train Acc 19.434904098510742 Val Acc 20.56502914428711\n",
      "Epoch: [5][ 0/36]\tTime  7.513 ( 7.513)\tData  7.066 ( 7.066)\tLoss 2.1522e+01 (2.1522e+01)\tAcc@1  19.14 ( 19.14)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [5][10/36]\tTime  0.151 ( 0.937)\tData  0.000 ( 0.758)\tLoss 1.4606e+01 (1.6907e+01)\tAcc@1  18.75 ( 19.67)\tAcc@5  65.23 ( 64.60)\n",
      "Epoch: [5][20/36]\tTime  1.091 ( 0.694)\tData  0.937 ( 0.528)\tLoss 1.2141e+01 (1.5875e+01)\tAcc@1  18.36 ( 19.92)\tAcc@5  74.61 ( 65.49)\n",
      "Epoch: [5][30/36]\tTime  0.151 ( 0.570)\tData  0.000 ( 0.409)\tLoss 9.5913e+00 (1.4310e+01)\tAcc@1  21.48 ( 20.07)\tAcc@5  68.36 ( 66.65)\n",
      " * Acc@1 21.990 Acc@5 63.069\n",
      "EPOCH 6 ENDED: LOSS train 14.218196143364311 LOSS val 14.124144538846384 Train Acc 19.867036819458008 Val Acc 21.990327835083008\n",
      "Epoch: [6][ 0/36]\tTime  7.580 ( 7.580)\tData  7.145 ( 7.145)\tLoss 1.4348e+01 (1.4348e+01)\tAcc@1  21.09 ( 21.09)\tAcc@5  60.94 ( 60.94)\n",
      "Epoch: [6][10/36]\tTime  0.151 ( 0.940)\tData  0.000 ( 0.763)\tLoss 1.5978e+01 (1.6404e+01)\tAcc@1  20.70 ( 20.95)\tAcc@5  66.80 ( 64.84)\n",
      "Epoch: [6][20/36]\tTime  0.918 ( 0.686)\tData  0.766 ( 0.521)\tLoss 1.3716e+01 (1.7323e+01)\tAcc@1  16.02 ( 19.62)\tAcc@5  63.67 ( 63.32)\n",
      "Epoch: [6][30/36]\tTime  0.151 ( 0.564)\tData  0.000 ( 0.403)\tLoss 1.7207e+01 (1.6682e+01)\tAcc@1  24.22 ( 19.97)\tAcc@5  64.06 ( 64.33)\n",
      " * Acc@1 22.067 Acc@5 69.509\n",
      "EPOCH 7 ENDED: LOSS train 16.839164112228435 LOSS val 13.071977787813967 Train Acc 19.988920211791992 Val Acc 22.066682815551758\n",
      "Epoch: [7][ 0/36]\tTime  7.297 ( 7.297)\tData  6.868 ( 6.868)\tLoss 1.2940e+01 (1.2940e+01)\tAcc@1  23.44 ( 23.44)\tAcc@5  75.39 ( 75.39)\n",
      "Epoch: [7][10/36]\tTime  0.152 ( 0.946)\tData  0.000 ( 0.766)\tLoss 1.3333e+01 (1.2913e+01)\tAcc@1  17.19 ( 20.17)\tAcc@5  62.50 ( 66.16)\n",
      "Epoch: [7][20/36]\tTime  0.835 ( 0.675)\tData  0.684 ( 0.509)\tLoss 1.1580e+01 (1.2297e+01)\tAcc@1  20.31 ( 20.48)\tAcc@5  63.28 ( 67.00)\n",
      "Epoch: [7][30/36]\tTime  0.152 ( 0.557)\tData  0.000 ( 0.396)\tLoss 7.6348e+00 (1.1425e+01)\tAcc@1  22.66 ( 21.98)\tAcc@5  67.58 ( 68.07)\n",
      " * Acc@1 22.576 Acc@5 65.386\n",
      "EPOCH 8 ENDED: LOSS train 11.098235407271874 LOSS val 9.043228174900396 Train Acc 22.027700424194336 Val Acc 22.57571792602539\n",
      "Epoch: [8][ 0/36]\tTime  7.293 ( 7.293)\tData  7.111 ( 7.111)\tLoss 8.1118e+00 (8.1118e+00)\tAcc@1  23.05 ( 23.05)\tAcc@5  67.58 ( 67.58)\n",
      "Epoch: [8][10/36]\tTime  0.152 ( 0.958)\tData  0.001 ( 0.796)\tLoss 1.6209e+01 (1.1547e+01)\tAcc@1  13.67 ( 20.95)\tAcc@5  67.97 ( 66.73)\n",
      "Epoch: [8][20/36]\tTime  0.357 ( 0.673)\tData  0.206 ( 0.516)\tLoss 1.7255e+01 (1.5275e+01)\tAcc@1  14.84 ( 19.51)\tAcc@5  65.62 ( 63.71)\n",
      "Epoch: [8][30/36]\tTime  0.151 ( 0.573)\tData  0.000 ( 0.418)\tLoss 1.3156e+01 (1.5894e+01)\tAcc@1  22.66 ( 19.78)\tAcc@5  73.83 ( 65.01)\n",
      " * Acc@1 19.674 Acc@5 69.865\n",
      "EPOCH 9 ENDED: LOSS train 15.514455255365768 LOSS val 10.971160928479101 Train Acc 20.288087844848633 Val Acc 19.674217224121094\n",
      "Epoch: [9][ 0/36]\tTime  7.610 ( 7.610)\tData  7.269 ( 7.269)\tLoss 1.0603e+01 (1.0603e+01)\tAcc@1  23.44 ( 23.44)\tAcc@5  67.19 ( 67.19)\n",
      "Epoch: [9][10/36]\tTime  0.153 ( 0.940)\tData  0.000 ( 0.770)\tLoss 1.1669e+01 (1.1475e+01)\tAcc@1  18.75 ( 20.49)\tAcc@5  71.09 ( 67.33)\n",
      "Epoch: [9][20/36]\tTime  1.126 ( 0.700)\tData  0.976 ( 0.539)\tLoss 8.9652e+00 (1.0473e+01)\tAcc@1  23.44 ( 22.38)\tAcc@5  72.66 ( 69.14)\n",
      "Epoch: [9][30/36]\tTime  0.153 ( 0.581)\tData  0.000 ( 0.423)\tLoss 1.4463e+01 (1.1272e+01)\tAcc@1  17.19 ( 21.93)\tAcc@5  62.89 ( 68.28)\n",
      " * Acc@1 26.521 Acc@5 69.661\n",
      "EPOCH 10 ENDED: LOSS train 11.66718191828424 LOSS val 12.289613134353086 Train Acc 21.628808975219727 Val Acc 26.520742416381836\n",
      "Epoch: [10][ 0/36]\tTime  7.507 ( 7.507)\tData  7.060 ( 7.060)\tLoss 9.7352e+00 (9.7352e+00)\tAcc@1  32.03 ( 32.03)\tAcc@5  68.75 ( 68.75)\n",
      "Epoch: [10][10/36]\tTime  0.151 ( 0.962)\tData  0.000 ( 0.783)\tLoss 9.6597e+00 (1.1154e+01)\tAcc@1  22.27 ( 24.54)\tAcc@5  73.05 ( 69.67)\n",
      "Epoch: [10][20/36]\tTime  0.973 ( 0.691)\tData  0.820 ( 0.525)\tLoss 1.2876e+01 (1.1746e+01)\tAcc@1  16.02 ( 22.45)\tAcc@5  66.80 ( 67.22)\n",
      "Epoch: [10][30/36]\tTime  0.153 ( 0.573)\tData  0.000 ( 0.411)\tLoss 1.9309e+01 (1.2991e+01)\tAcc@1  23.83 ( 21.69)\tAcc@5  63.67 ( 66.77)\n",
      " * Acc@1 21.329 Acc@5 65.589\n",
      "EPOCH 11 ENDED: LOSS train 13.201548683398979 LOSS val 12.117865509039028 Train Acc 21.418283462524414 Val Acc 21.328582763671875\n",
      "Epoch: [11][ 0/36]\tTime  7.645 ( 7.645)\tData  7.299 ( 7.299)\tLoss 1.3232e+01 (1.3232e+01)\tAcc@1  24.22 ( 24.22)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [11][10/36]\tTime  0.172 ( 0.965)\tData  0.001 ( 0.789)\tLoss 1.2334e+01 (1.3921e+01)\tAcc@1  26.17 ( 20.81)\tAcc@5  69.92 ( 63.78)\n",
      "Epoch: [11][20/36]\tTime  0.880 ( 0.723)\tData  0.730 ( 0.558)\tLoss 8.0443e+00 (1.1647e+01)\tAcc@1  28.52 ( 22.36)\tAcc@5  70.31 ( 67.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][30/36]\tTime  0.151 ( 0.590)\tData  0.000 ( 0.430)\tLoss 1.5709e+01 (1.2042e+01)\tAcc@1  18.75 ( 21.64)\tAcc@5  56.25 ( 66.65)\n",
      " * Acc@1 14.482 Acc@5 59.939\n",
      "EPOCH 12 ENDED: LOSS train 12.683971067941089 LOSS val 28.340907461195993 Train Acc 21.329639434814453 Val Acc 14.482056617736816\n",
      "Epoch: [12][ 0/36]\tTime  7.657 ( 7.657)\tData  7.243 ( 7.243)\tLoss 2.9789e+01 (2.9789e+01)\tAcc@1  13.28 ( 13.28)\tAcc@5  58.59 ( 58.59)\n",
      "Epoch: [12][10/36]\tTime  0.151 ( 0.944)\tData  0.000 ( 0.768)\tLoss 1.0441e+01 (1.9005e+01)\tAcc@1  28.52 ( 20.42)\tAcc@5  71.09 ( 64.20)\n",
      "Epoch: [12][20/36]\tTime  0.620 ( 0.670)\tData  0.470 ( 0.506)\tLoss 9.6752e+00 (1.5266e+01)\tAcc@1  21.88 ( 22.04)\tAcc@5  68.36 ( 67.08)\n",
      "Epoch: [12][30/36]\tTime  0.151 ( 0.560)\tData  0.000 ( 0.399)\tLoss 7.7842e+00 (1.2886e+01)\tAcc@1  23.05 ( 22.66)\tAcc@5  75.39 ( 68.85)\n",
      " * Acc@1 22.194 Acc@5 72.461\n",
      "EPOCH 13 ENDED: LOSS train 12.149347737911995 LOSS val 8.236341570011682 Train Acc 22.8919677734375 Val Acc 22.193941116333008\n",
      "Epoch: [13][ 0/36]\tTime  7.591 ( 7.591)\tData  7.083 ( 7.083)\tLoss 8.2631e+00 (8.2631e+00)\tAcc@1  20.31 ( 20.31)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [13][10/36]\tTime  0.152 ( 0.934)\tData  0.000 ( 0.750)\tLoss 1.2177e+01 (9.9325e+00)\tAcc@1  18.36 ( 21.27)\tAcc@5  58.98 ( 68.08)\n",
      "Epoch: [13][20/36]\tTime  0.894 ( 0.690)\tData  0.743 ( 0.521)\tLoss 1.6078e+01 (1.2780e+01)\tAcc@1  17.19 ( 20.46)\tAcc@5  60.16 ( 65.25)\n",
      "Epoch: [13][30/36]\tTime  0.151 ( 0.577)\tData  0.000 ( 0.413)\tLoss 1.8404e+01 (1.4026e+01)\tAcc@1  18.75 ( 20.20)\tAcc@5  57.81 ( 65.61)\n",
      " * Acc@1 20.718 Acc@5 64.546\n",
      "EPOCH 14 ENDED: LOSS train 14.256268253484922 LOSS val 13.80025719467633 Train Acc 20.33241081237793 Val Acc 20.71773910522461\n",
      "Epoch: [14][ 0/36]\tTime  7.445 ( 7.445)\tData  7.031 ( 7.031)\tLoss 1.2231e+01 (1.2231e+01)\tAcc@1  21.48 ( 21.48)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [14][10/36]\tTime  0.153 ( 0.937)\tData  0.000 ( 0.761)\tLoss 1.0877e+01 (1.2348e+01)\tAcc@1  24.22 ( 21.41)\tAcc@5  71.48 ( 68.04)\n",
      "Epoch: [14][20/36]\tTime  0.827 ( 0.676)\tData  0.676 ( 0.511)\tLoss 1.1494e+01 (1.1312e+01)\tAcc@1  19.92 ( 22.12)\tAcc@5  66.80 ( 69.18)\n",
      "Epoch: [14][30/36]\tTime  0.152 ( 0.560)\tData  0.000 ( 0.400)\tLoss 1.1206e+01 (1.1480e+01)\tAcc@1  25.78 ( 21.62)\tAcc@5  64.45 ( 68.38)\n",
      " * Acc@1 21.710 Acc@5 64.597\n",
      "EPOCH 15 ENDED: LOSS train 11.66075670247593 LOSS val 13.616709579429909 Train Acc 20.94182777404785 Val Acc 21.710357666015625\n",
      "Epoch: [15][ 0/36]\tTime  7.477 ( 7.477)\tData  7.022 ( 7.022)\tLoss 1.4123e+01 (1.4123e+01)\tAcc@1  22.66 ( 22.66)\tAcc@5  60.55 ( 60.55)\n",
      "Epoch: [15][10/36]\tTime  0.154 ( 0.963)\tData  0.001 ( 0.782)\tLoss 8.7246e+00 (1.1972e+01)\tAcc@1  21.88 ( 21.24)\tAcc@5  71.48 ( 68.04)\n",
      "Epoch: [15][20/36]\tTime  1.028 ( 0.711)\tData  0.878 ( 0.544)\tLoss 5.5385e+00 (9.6725e+00)\tAcc@1  25.78 ( 22.86)\tAcc@5  69.92 ( 70.09)\n",
      "Epoch: [15][30/36]\tTime  0.151 ( 0.584)\tData  0.000 ( 0.421)\tLoss 8.8328e+00 (9.2639e+00)\tAcc@1  25.39 ( 22.86)\tAcc@5  65.62 ( 69.90)\n",
      " * Acc@1 21.074 Acc@5 72.436\n",
      "EPOCH 16 ENDED: LOSS train 9.240182322451943 LOSS val 7.579192229280887 Train Acc 23.168975830078125 Val Acc 21.074064254760742\n",
      "Epoch: [16][ 0/36]\tTime  7.496 ( 7.496)\tData  7.005 ( 7.005)\tLoss 7.6262e+00 (7.6262e+00)\tAcc@1  21.88 ( 21.88)\tAcc@5  70.31 ( 70.31)\n",
      "Epoch: [16][10/36]\tTime  0.151 ( 0.940)\tData  0.000 ( 0.758)\tLoss 1.8295e+01 (1.3458e+01)\tAcc@1  14.45 ( 20.24)\tAcc@5  63.28 ( 64.28)\n",
      "Epoch: [16][20/36]\tTime  0.795 ( 0.696)\tData  0.644 ( 0.528)\tLoss 1.2655e+01 (1.3593e+01)\tAcc@1  20.31 ( 20.48)\tAcc@5  62.89 ( 64.66)\n",
      "Epoch: [16][30/36]\tTime  0.152 ( 0.573)\tData  0.000 ( 0.410)\tLoss 9.5904e+00 (1.2459e+01)\tAcc@1  25.00 ( 21.30)\tAcc@5  76.17 ( 66.33)\n",
      " * Acc@1 21.252 Acc@5 67.778\n",
      "EPOCH 17 ENDED: LOSS train 12.292945794187425 LOSS val 12.524056488395374 Train Acc 21.063711166381836 Val Acc 21.252225875854492\n",
      "Epoch: [17][ 0/36]\tTime  7.415 ( 7.415)\tData  7.015 ( 7.015)\tLoss 1.0976e+01 (1.0976e+01)\tAcc@1  21.48 ( 21.48)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [17][10/36]\tTime  0.150 ( 0.949)\tData  0.000 ( 0.774)\tLoss 1.3763e+01 (1.3077e+01)\tAcc@1  18.75 ( 21.09)\tAcc@5  69.53 ( 68.54)\n",
      "Epoch: [17][20/36]\tTime  0.337 ( 0.677)\tData  0.185 ( 0.512)\tLoss 9.8522e+00 (1.2059e+01)\tAcc@1  25.00 ( 22.82)\tAcc@5  69.92 ( 69.35)\n",
      "Epoch: [17][30/36]\tTime  0.150 ( 0.580)\tData  0.000 ( 0.419)\tLoss 9.8711e+00 (1.1867e+01)\tAcc@1  17.58 ( 22.77)\tAcc@5  61.72 ( 68.40)\n",
      " * Acc@1 21.583 Acc@5 65.182\n",
      "EPOCH 18 ENDED: LOSS train 11.588690858032564 LOSS val 12.416905762130416 Train Acc 22.73684310913086 Val Acc 21.583099365234375\n",
      "Epoch: [18][ 0/36]\tTime  7.499 ( 7.499)\tData  7.167 ( 7.167)\tLoss 1.1208e+01 (1.1208e+01)\tAcc@1  21.88 ( 21.88)\tAcc@5  59.38 ( 59.38)\n",
      "Epoch: [18][10/36]\tTime  0.152 ( 0.956)\tData  0.000 ( 0.785)\tLoss 7.7044e+00 (9.9903e+00)\tAcc@1  22.27 ( 23.22)\tAcc@5  76.56 ( 67.86)\n",
      "Epoch: [18][20/36]\tTime  0.941 ( 0.711)\tData  0.790 ( 0.547)\tLoss 1.3422e+01 (1.0673e+01)\tAcc@1  17.97 ( 21.91)\tAcc@5  67.19 ( 67.04)\n",
      "Epoch: [18][30/36]\tTime  0.151 ( 0.595)\tData  0.000 ( 0.435)\tLoss 1.0997e+01 (1.0769e+01)\tAcc@1  18.75 ( 21.42)\tAcc@5  71.48 ( 68.03)\n",
      " * Acc@1 22.652 Acc@5 70.043\n",
      "EPOCH 19 ENDED: LOSS train 11.128237837899755 LOSS val 13.042361848801564 Train Acc 20.952909469604492 Val Acc 22.65207290649414\n",
      "Epoch: [19][ 0/36]\tTime  7.535 ( 7.535)\tData  7.084 ( 7.084)\tLoss 1.2107e+01 (1.2107e+01)\tAcc@1  19.92 ( 19.92)\tAcc@5  67.58 ( 67.58)\n",
      "Epoch: [19][10/36]\tTime  0.256 ( 0.971)\tData  0.104 ( 0.790)\tLoss 8.7717e+00 (1.2933e+01)\tAcc@1  27.73 ( 22.05)\tAcc@5  75.00 ( 68.22)\n",
      "Epoch: [19][20/36]\tTime  0.521 ( 0.673)\tData  0.369 ( 0.506)\tLoss 1.4422e+01 (1.3086e+01)\tAcc@1  17.58 ( 21.47)\tAcc@5  66.80 ( 68.68)\n",
      "Epoch: [19][30/36]\tTime  0.301 ( 0.587)\tData  0.149 ( 0.423)\tLoss 1.2545e+01 (1.3315e+01)\tAcc@1  19.53 ( 20.97)\tAcc@5  65.62 ( 67.82)\n",
      " * Acc@1 22.168 Acc@5 69.891\n",
      "EPOCH 20 ENDED: LOSS train 13.383100015951985 LOSS val 13.546384556052582 Train Acc 20.64266014099121 Val Acc 22.168489456176758\n",
      "Epoch: [20][ 0/36]\tTime  7.396 ( 7.396)\tData  6.990 ( 6.990)\tLoss 1.2530e+01 (1.2530e+01)\tAcc@1  25.39 ( 25.39)\tAcc@5  69.53 ( 69.53)\n",
      "Epoch: [20][10/36]\tTime  0.151 ( 0.945)\tData  0.000 ( 0.770)\tLoss 1.2197e+01 (1.3851e+01)\tAcc@1  26.17 ( 22.44)\tAcc@5  64.45 ( 68.47)\n",
      "Epoch: [20][20/36]\tTime  0.989 ( 0.688)\tData  0.838 ( 0.524)\tLoss 9.4088e+00 (1.1796e+01)\tAcc@1  23.44 ( 22.69)\tAcc@5  70.70 ( 68.47)\n",
      "Epoch: [20][30/36]\tTime  0.152 ( 0.569)\tData  0.000 ( 0.409)\tLoss 9.2692e+00 (1.0774e+01)\tAcc@1  22.27 ( 23.22)\tAcc@5  69.14 ( 69.73)\n",
      " * Acc@1 23.314 Acc@5 69.305\n",
      "EPOCH 21 ENDED: LOSS train 10.387171700073411 LOSS val 7.083806325777034 Train Acc 23.645429611206055 Val Acc 23.313819885253906\n",
      "Epoch: [21][ 0/36]\tTime  7.401 ( 7.401)\tData  6.995 ( 6.995)\tLoss 6.7277e+00 (6.7277e+00)\tAcc@1  23.44 ( 23.44)\tAcc@5  68.36 ( 68.36)\n",
      "Epoch: [21][10/36]\tTime  0.150 ( 0.969)\tData  0.000 ( 0.794)\tLoss 9.7520e+00 (8.6952e+00)\tAcc@1  22.66 ( 23.05)\tAcc@5  70.31 ( 70.81)\n",
      "Epoch: [21][20/36]\tTime  0.843 ( 0.699)\tData  0.691 ( 0.535)\tLoss 2.3214e+01 (1.0692e+01)\tAcc@1  16.02 ( 21.50)\tAcc@5  58.20 ( 68.23)\n",
      "Epoch: [21][30/36]\tTime  0.153 ( 0.581)\tData  0.000 ( 0.421)\tLoss 1.3339e+01 (1.1794e+01)\tAcc@1  19.92 ( 21.16)\tAcc@5  66.02 ( 67.28)\n",
      " * Acc@1 18.885 Acc@5 67.676\n",
      "EPOCH 22 ENDED: LOSS train 11.6674890416745 LOSS val 11.559103517624585 Train Acc 21.28531837463379 Val Acc 18.885211944580078\n",
      "Epoch: [22][ 0/36]\tTime  7.408 ( 7.408)\tData  7.046 ( 7.046)\tLoss 1.2108e+01 (1.2108e+01)\tAcc@1  17.19 ( 17.19)\tAcc@5  71.09 ( 71.09)\n",
      "Epoch: [22][10/36]\tTime  0.153 ( 0.957)\tData  0.001 ( 0.786)\tLoss 1.5362e+01 (1.1882e+01)\tAcc@1  17.97 ( 22.37)\tAcc@5  64.45 ( 68.68)\n",
      "Epoch: [22][20/36]\tTime  1.217 ( 0.705)\tData  1.063 ( 0.542)\tLoss 9.8177e+00 (1.1446e+01)\tAcc@1  30.47 ( 22.99)\tAcc@5  73.05 ( 69.64)\n",
      "Epoch: [22][30/36]\tTime  0.151 ( 0.582)\tData  0.000 ( 0.423)\tLoss 1.2364e+01 (1.1358e+01)\tAcc@1  22.27 ( 22.44)\tAcc@5  67.19 ( 68.72)\n",
      " * Acc@1 16.798 Acc@5 62.611\n",
      "EPOCH 23 ENDED: LOSS train 11.644926539392022 LOSS val 14.978316959032547 Train Acc 21.83933448791504 Val Acc 16.798166275024414\n",
      "Epoch: [23][ 0/36]\tTime  7.305 ( 7.305)\tData  6.858 ( 6.858)\tLoss 1.2703e+01 (1.2703e+01)\tAcc@1  16.41 ( 16.41)\tAcc@5  63.28 ( 63.28)\n",
      "Epoch: [23][10/36]\tTime  0.153 ( 0.957)\tData  0.001 ( 0.777)\tLoss 1.0696e+01 (1.3613e+01)\tAcc@1  18.75 ( 19.67)\tAcc@5  64.06 ( 62.36)\n",
      "Epoch: [23][20/36]\tTime  0.661 ( 0.676)\tData  0.509 ( 0.510)\tLoss 7.4274e+00 (1.0987e+01)\tAcc@1  24.22 ( 21.69)\tAcc@5  69.14 ( 66.50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][30/36]\tTime  0.151 ( 0.572)\tData  0.000 ( 0.410)\tLoss 1.0128e+01 (1.0312e+01)\tAcc@1  20.70 ( 22.14)\tAcc@5  66.80 ( 67.25)\n",
      " * Acc@1 18.503 Acc@5 68.058\n",
      "EPOCH 24 ENDED: LOSS train 10.288529249809455 LOSS val 8.55571746692672 Train Acc 22.08310317993164 Val Acc 18.503435134887695\n",
      "Epoch: [24][ 0/36]\tTime  7.433 ( 7.433)\tData  7.095 ( 7.095)\tLoss 7.9473e+00 (7.9473e+00)\tAcc@1  19.14 ( 19.14)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [24][10/36]\tTime  0.184 ( 0.983)\tData  0.000 ( 0.806)\tLoss 9.0503e+00 (8.7796e+00)\tAcc@1  26.95 ( 23.72)\tAcc@5  69.14 ( 66.26)\n",
      "Epoch: [24][20/36]\tTime  0.953 ( 0.724)\tData  0.803 ( 0.556)\tLoss 1.1836e+01 (1.0824e+01)\tAcc@1  20.70 ( 22.04)\tAcc@5  64.06 ( 66.13)\n",
      "Epoch: [24][30/36]\tTime  0.151 ( 0.594)\tData  0.000 ( 0.431)\tLoss 1.1128e+01 (1.0960e+01)\tAcc@1  21.88 ( 22.08)\tAcc@5  70.70 ( 66.60)\n",
      " * Acc@1 23.772 Acc@5 69.636\n",
      "EPOCH 25 ENDED: LOSS train 10.715079062612434 LOSS val 9.84212564687809 Train Acc 22.09418296813965 Val Acc 23.77195167541504\n",
      "Epoch: [25][ 0/36]\tTime  7.543 ( 7.543)\tData  7.137 ( 7.137)\tLoss 1.0128e+01 (1.0128e+01)\tAcc@1  18.75 ( 18.75)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [25][10/36]\tTime  0.151 ( 0.954)\tData  0.000 ( 0.777)\tLoss 1.0846e+01 (8.3318e+00)\tAcc@1  21.48 ( 23.93)\tAcc@5  65.23 ( 71.41)\n",
      "Epoch: [25][20/36]\tTime  1.103 ( 0.701)\tData  0.951 ( 0.536)\tLoss 8.4927e+00 (8.6572e+00)\tAcc@1  25.00 ( 23.60)\tAcc@5  73.44 ( 70.65)\n",
      "Epoch: [25][30/36]\tTime  0.152 ( 0.572)\tData  0.000 ( 0.412)\tLoss 1.2584e+01 (9.1666e+00)\tAcc@1  21.48 ( 23.35)\tAcc@5  61.72 ( 70.01)\n",
      " * Acc@1 17.791 Acc@5 66.327\n",
      "EPOCH 26 ENDED: LOSS train 9.889545867687447 LOSS val 18.567945691883246 Train Acc 23.379501342773438 Val Acc 17.790786743164062\n",
      "Epoch: [26][ 0/36]\tTime  7.292 ( 7.292)\tData  6.864 ( 6.864)\tLoss 2.1751e+01 (2.1751e+01)\tAcc@1  17.58 ( 17.58)\tAcc@5  61.33 ( 61.33)\n",
      "Epoch: [26][10/36]\tTime  0.254 ( 0.920)\tData  0.104 ( 0.743)\tLoss 7.2501e+00 (1.1575e+01)\tAcc@1  25.78 ( 23.33)\tAcc@5  74.22 ( 69.39)\n",
      "Epoch: [26][20/36]\tTime  0.982 ( 0.676)\tData  0.830 ( 0.511)\tLoss 7.9991e+00 (1.0197e+01)\tAcc@1  21.48 ( 23.87)\tAcc@5  71.88 ( 70.55)\n",
      "Epoch: [26][30/36]\tTime  0.263 ( 0.565)\tData  0.113 ( 0.405)\tLoss 5.5795e+00 (8.9599e+00)\tAcc@1  22.66 ( 23.98)\tAcc@5  78.12 ( 71.69)\n",
      " * Acc@1 22.652 Acc@5 74.090\n",
      "EPOCH 27 ENDED: LOSS train 8.72556973386009 LOSS val 5.895613814893039 Train Acc 24.121883392333984 Val Acc 22.65207290649414\n",
      "Epoch: [27][ 0/36]\tTime  7.595 ( 7.595)\tData  7.084 ( 7.084)\tLoss 5.6989e+00 (5.6989e+00)\tAcc@1  27.73 ( 27.73)\tAcc@5  76.56 ( 76.56)\n",
      "Epoch: [27][10/36]\tTime  0.152 ( 0.952)\tData  0.000 ( 0.767)\tLoss 6.8776e+00 (7.0241e+00)\tAcc@1  20.70 ( 24.93)\tAcc@5  68.36 ( 71.48)\n",
      "Epoch: [27][20/36]\tTime  0.871 ( 0.686)\tData  0.720 ( 0.518)\tLoss 1.4113e+01 (9.3899e+00)\tAcc@1  29.30 ( 23.23)\tAcc@5  67.58 ( 68.69)\n",
      "Epoch: [27][30/36]\tTime  0.152 ( 0.564)\tData  0.001 ( 0.401)\tLoss 8.7365e+00 (9.9451e+00)\tAcc@1  19.14 ( 23.00)\tAcc@5  67.58 ( 68.31)\n",
      " * Acc@1 24.765 Acc@5 65.157\n",
      "EPOCH 28 ENDED: LOSS train 9.901556531213988 LOSS val 11.65988285239946 Train Acc 23.157894134521484 Val Acc 24.764570236206055\n",
      "Epoch: [28][ 0/36]\tTime  7.805 ( 7.805)\tData  7.326 ( 7.326)\tLoss 1.0876e+01 (1.0876e+01)\tAcc@1  23.83 ( 23.83)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [28][10/36]\tTime  0.152 ( 0.990)\tData  0.000 ( 0.809)\tLoss 8.9996e+00 (9.1469e+00)\tAcc@1  24.22 ( 23.05)\tAcc@5  63.67 ( 68.86)\n",
      "Epoch: [28][20/36]\tTime  0.986 ( 0.711)\tData  0.835 ( 0.544)\tLoss 9.3808e+00 (8.8022e+00)\tAcc@1  18.75 ( 22.88)\tAcc@5  67.19 ( 68.95)\n",
      "Epoch: [28][30/36]\tTime  0.157 ( 0.594)\tData  0.000 ( 0.431)\tLoss 8.3512e+00 (8.3867e+00)\tAcc@1  21.88 ( 23.40)\tAcc@5  65.23 ( 69.91)\n",
      " * Acc@1 25.095 Acc@5 71.214\n",
      "EPOCH 29 ENDED: LOSS train 8.29688882716805 LOSS val 7.065700559366991 Train Acc 23.246538162231445 Val Acc 25.095443725585938\n",
      "Epoch: [29][ 0/36]\tTime  7.484 ( 7.484)\tData  7.056 ( 7.056)\tLoss 7.0150e+00 (7.0150e+00)\tAcc@1  26.17 ( 26.17)\tAcc@5  72.66 ( 72.66)\n",
      "Epoch: [29][10/36]\tTime  0.247 ( 0.955)\tData  0.083 ( 0.776)\tLoss 1.3516e+01 (9.4777e+00)\tAcc@1  19.53 ( 22.90)\tAcc@5  59.77 ( 67.79)\n",
      "Epoch: [29][20/36]\tTime  0.922 ( 0.697)\tData  0.770 ( 0.531)\tLoss 1.2815e+01 (1.0528e+01)\tAcc@1  25.78 ( 21.89)\tAcc@5  65.62 ( 66.72)\n",
      "Epoch: [29][30/36]\tTime  0.151 ( 0.579)\tData  0.000 ( 0.417)\tLoss 9.4084e+00 (1.1169e+01)\tAcc@1  25.39 ( 21.79)\tAcc@5  66.80 ( 66.37)\n",
      " * Acc@1 22.168 Acc@5 67.600\n",
      "EPOCH 30 ENDED: LOSS train 11.12096172068588 LOSS val 13.735973523056941 Train Acc 21.75069236755371 Val Acc 22.168489456176758\n",
      "Epoch: [30][ 0/36]\tTime  7.605 ( 7.605)\tData  7.181 ( 7.181)\tLoss 1.2794e+01 (1.2794e+01)\tAcc@1  23.83 ( 23.83)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [30][10/36]\tTime  0.152 ( 0.972)\tData  0.000 ( 0.792)\tLoss 8.7710e+00 (1.0467e+01)\tAcc@1  28.12 ( 22.48)\tAcc@5  74.22 ( 68.22)\n",
      "Epoch: [30][20/36]\tTime  1.109 ( 0.711)\tData  0.940 ( 0.542)\tLoss 6.5227e+00 (9.1308e+00)\tAcc@1  30.86 ( 23.12)\tAcc@5  72.27 ( 69.66)\n",
      "Epoch: [30][30/36]\tTime  0.151 ( 0.596)\tData  0.000 ( 0.431)\tLoss 9.3998e+00 (9.3261e+00)\tAcc@1  21.88 ( 23.22)\tAcc@5  68.75 ( 69.77)\n",
      " * Acc@1 25.426 Acc@5 67.676\n",
      "EPOCH 31 ENDED: LOSS train 9.195877803580583 LOSS val 8.482109618265836 Train Acc 23.656509399414062 Val Acc 25.42631721496582\n",
      "Epoch: [31][ 0/36]\tTime  7.578 ( 7.578)\tData  7.183 ( 7.183)\tLoss 8.7495e+00 (8.7495e+00)\tAcc@1  24.22 ( 24.22)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [31][10/36]\tTime  0.153 ( 0.949)\tData  0.001 ( 0.775)\tLoss 8.3407e+00 (7.6825e+00)\tAcc@1  22.27 ( 24.79)\tAcc@5  74.22 ( 71.84)\n",
      "Epoch: [31][20/36]\tTime  0.926 ( 0.675)\tData  0.774 ( 0.511)\tLoss 6.7663e+00 (7.3249e+00)\tAcc@1  24.22 ( 25.07)\tAcc@5  75.00 ( 72.27)\n",
      "Epoch: [31][30/36]\tTime  0.154 ( 0.575)\tData  0.000 ( 0.415)\tLoss 6.1554e+00 (7.2503e+00)\tAcc@1  27.34 ( 25.18)\tAcc@5  69.53 ( 72.29)\n",
      " * Acc@1 22.168 Acc@5 64.953\n",
      "EPOCH 32 ENDED: LOSS train 7.187516555099276 LOSS val 7.4616550679545455 Train Acc 25.11911392211914 Val Acc 22.168489456176758\n",
      "Epoch: [32][ 0/36]\tTime  7.440 ( 7.440)\tData  6.999 ( 6.999)\tLoss 7.1203e+00 (7.1203e+00)\tAcc@1  22.66 ( 22.66)\tAcc@5  67.19 ( 67.19)\n",
      "Epoch: [32][10/36]\tTime  0.152 ( 0.953)\tData  0.000 ( 0.776)\tLoss 9.2509e+00 (7.9526e+00)\tAcc@1  22.66 ( 23.33)\tAcc@5  68.75 ( 69.67)\n",
      "Epoch: [32][20/36]\tTime  1.042 ( 0.709)\tData  0.888 ( 0.533)\tLoss 1.1996e+01 (9.2291e+00)\tAcc@1  17.19 ( 21.91)\tAcc@5  63.28 ( 68.01)\n",
      "Epoch: [32][30/36]\tTime  0.154 ( 0.591)\tData  0.000 ( 0.421)\tLoss 6.7112e+00 (9.1421e+00)\tAcc@1  24.61 ( 22.62)\tAcc@5  75.00 ( 68.36)\n",
      " * Acc@1 22.881 Acc@5 69.941\n",
      "EPOCH 33 ENDED: LOSS train 8.879961269674538 LOSS val 6.8675251602368 Train Acc 22.847644805908203 Val Acc 22.881139755249023\n",
      "Epoch: [33][ 0/36]\tTime  8.000 ( 8.000)\tData  7.620 ( 7.620)\tLoss 7.2044e+00 (7.2044e+00)\tAcc@1  25.00 ( 25.00)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [33][10/36]\tTime  0.151 ( 1.012)\tData  0.000 ( 0.830)\tLoss 6.6613e+00 (6.9968e+00)\tAcc@1  20.70 ( 26.49)\tAcc@5  70.70 ( 70.92)\n",
      "Epoch: [33][20/36]\tTime  0.995 ( 0.729)\tData  0.833 ( 0.555)\tLoss 4.3839e+00 (6.1368e+00)\tAcc@1  30.86 ( 26.41)\tAcc@5  76.56 ( 72.64)\n",
      "Epoch: [33][30/36]\tTime  0.233 ( 0.605)\tData  0.081 ( 0.437)\tLoss 5.9788e+00 (5.8139e+00)\tAcc@1  29.69 ( 27.10)\tAcc@5  73.44 ( 73.35)\n",
      " * Acc@1 20.590 Acc@5 73.072\n",
      "EPOCH 34 ENDED: LOSS train 5.697659628886595 LOSS val 6.274846062523077 Train Acc 27.080331802368164 Val Acc 20.59048080444336\n",
      "Epoch: [34][ 0/36]\tTime  7.628 ( 7.628)\tData  7.212 ( 7.212)\tLoss 6.4176e+00 (6.4176e+00)\tAcc@1  17.19 ( 17.19)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [34][10/36]\tTime  0.151 ( 0.970)\tData  0.000 ( 0.794)\tLoss 1.0850e+01 (7.7828e+00)\tAcc@1  19.92 ( 22.87)\tAcc@5  60.55 ( 68.32)\n",
      "Epoch: [34][20/36]\tTime  0.617 ( 0.696)\tData  0.464 ( 0.531)\tLoss 1.0277e+01 (9.2628e+00)\tAcc@1  23.05 ( 22.25)\tAcc@5  69.14 ( 68.23)\n",
      "Epoch: [34][30/36]\tTime  0.151 ( 0.583)\tData  0.000 ( 0.423)\tLoss 6.5280e+00 (1.0000e+01)\tAcc@1  24.22 ( 21.66)\tAcc@5  70.31 ( 67.30)\n",
      " * Acc@1 21.710 Acc@5 69.611\n",
      "EPOCH 35 ENDED: LOSS train 9.936649473671107 LOSS val 7.584450802811535 Train Acc 21.950138092041016 Val Acc 21.710357666015625\n",
      "Epoch: [35][ 0/36]\tTime  7.938 ( 7.938)\tData  7.487 ( 7.487)\tLoss 6.3622e+00 (6.3622e+00)\tAcc@1  20.31 ( 20.31)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [35][10/36]\tTime  0.175 ( 0.995)\tData  0.000 ( 0.795)\tLoss 1.4626e+01 (1.1512e+01)\tAcc@1  12.50 ( 20.53)\tAcc@5  67.19 ( 66.62)\n",
      "Epoch: [35][20/36]\tTime  0.945 ( 0.722)\tData  0.770 ( 0.533)\tLoss 9.8516e+00 (1.1659e+01)\tAcc@1  21.88 ( 21.13)\tAcc@5  62.11 ( 65.55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][30/36]\tTime  0.153 ( 0.595)\tData  0.000 ( 0.416)\tLoss 6.9892e+00 (1.0846e+01)\tAcc@1  23.44 ( 21.69)\tAcc@5  67.97 ( 67.59)\n",
      " * Acc@1 23.772 Acc@5 68.083\n",
      "EPOCH 36 ENDED: LOSS train 10.3244233393471 LOSS val 8.678256783421089 Train Acc 22.00554084777832 Val Acc 23.77195167541504\n",
      "Epoch: [36][ 0/36]\tTime  8.364 ( 8.364)\tData  8.016 ( 8.016)\tLoss 8.3860e+00 (8.3860e+00)\tAcc@1  24.61 ( 24.61)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [36][10/36]\tTime  0.151 ( 1.034)\tData  0.000 ( 0.863)\tLoss 5.1821e+00 (6.4603e+00)\tAcc@1  25.00 ( 24.82)\tAcc@5  72.66 ( 71.70)\n",
      "Epoch: [36][20/36]\tTime  0.893 ( 0.745)\tData  0.743 ( 0.583)\tLoss 4.9793e+00 (5.6977e+00)\tAcc@1  27.73 ( 26.02)\tAcc@5  73.83 ( 73.42)\n",
      "Epoch: [36][30/36]\tTime  0.153 ( 0.609)\tData  0.000 ( 0.449)\tLoss 4.9799e+00 (5.4106e+00)\tAcc@1  28.12 ( 26.81)\tAcc@5  73.44 ( 73.65)\n",
      " * Acc@1 26.877 Acc@5 77.501\n",
      "EPOCH 37 ENDED: LOSS train 5.392438888972486 LOSS val 4.708356725923825 Train Acc 27.19113540649414 Val Acc 26.87706756591797\n",
      "Epoch: [37][ 0/36]\tTime  7.561 ( 7.561)\tData  7.211 ( 7.211)\tLoss 4.1443e+00 (4.1443e+00)\tAcc@1  31.64 ( 31.64)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [37][10/36]\tTime  0.150 ( 0.971)\tData  0.000 ( 0.801)\tLoss 7.1127e+00 (5.2084e+00)\tAcc@1  25.78 ( 25.28)\tAcc@5  71.09 ( 74.68)\n",
      "Epoch: [37][20/36]\tTime  0.725 ( 0.681)\tData  0.575 ( 0.520)\tLoss 1.1192e+01 (6.7055e+00)\tAcc@1  24.61 ( 24.44)\tAcc@5  60.55 ( 69.92)\n",
      "Epoch: [37][30/36]\tTime  0.291 ( 0.579)\tData  0.140 ( 0.420)\tLoss 6.4689e+00 (7.4136e+00)\tAcc@1  23.44 ( 24.58)\tAcc@5  75.78 ( 70.49)\n",
      " * Acc@1 18.096 Acc@5 67.345\n",
      "EPOCH 38 ENDED: LOSS train 7.321201732350188 LOSS val 8.927381937605153 Train Acc 24.70914077758789 Val Acc 18.096206665039062\n",
      "Epoch: [38][ 0/36]\tTime  7.607 ( 7.607)\tData  7.198 ( 7.198)\tLoss 9.3574e+00 (9.3574e+00)\tAcc@1  18.75 ( 18.75)\tAcc@5  65.62 ( 65.62)\n",
      "Epoch: [38][10/36]\tTime  0.174 ( 0.952)\tData  0.000 ( 0.770)\tLoss 1.0815e+01 (9.9724e+00)\tAcc@1  21.88 ( 21.84)\tAcc@5  60.16 ( 63.88)\n",
      "Epoch: [38][20/36]\tTime  0.668 ( 0.687)\tData  0.516 ( 0.519)\tLoss 8.5578e+00 (9.2178e+00)\tAcc@1  22.66 ( 22.25)\tAcc@5  69.14 ( 65.94)\n",
      "Epoch: [38][30/36]\tTime  0.174 ( 0.595)\tData  0.000 ( 0.425)\tLoss 6.0579e+00 (8.6048e+00)\tAcc@1  30.47 ( 22.86)\tAcc@5  73.83 ( 67.25)\n",
      " * Acc@1 23.848 Acc@5 72.385\n",
      "EPOCH 39 ENDED: LOSS train 8.369617797661355 LOSS val 7.872558667660367 Train Acc 22.626039505004883 Val Acc 23.84830665588379\n",
      "Epoch: [39][ 0/36]\tTime  7.598 ( 7.598)\tData  7.275 ( 7.275)\tLoss 7.9910e+00 (7.9910e+00)\tAcc@1  25.39 ( 25.39)\tAcc@5  68.36 ( 68.36)\n",
      "Epoch: [39][10/36]\tTime  0.192 ( 0.979)\tData  0.000 ( 0.787)\tLoss 7.4722e+00 (7.1999e+00)\tAcc@1  16.02 ( 23.08)\tAcc@5  71.48 ( 70.74)\n",
      "Epoch: [39][20/36]\tTime  1.511 ( 0.739)\tData  1.361 ( 0.563)\tLoss 4.5516e+00 (6.2375e+00)\tAcc@1  24.61 ( 24.78)\tAcc@5  72.66 ( 72.86)\n",
      "Epoch: [39][30/36]\tTime  0.151 ( 0.608)\tData  0.000 ( 0.440)\tLoss 7.9829e+00 (6.1435e+00)\tAcc@1  16.02 ( 24.33)\tAcc@5  71.09 ( 72.66)\n",
      " * Acc@1 25.732 Acc@5 73.403\n",
      "EPOCH 40 ENDED: LOSS train 6.1641479072676475 LOSS val 6.211793777747869 Train Acc 24.454294204711914 Val Acc 25.73173713684082\n",
      "Epoch: [40][ 0/36]\tTime  7.769 ( 7.769)\tData  7.307 ( 7.307)\tLoss 5.9858e+00 (5.9858e+00)\tAcc@1  26.17 ( 26.17)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [40][10/36]\tTime  0.171 ( 0.983)\tData  0.000 ( 0.781)\tLoss 8.4188e+00 (7.1332e+00)\tAcc@1  25.39 ( 24.29)\tAcc@5  62.11 ( 69.71)\n",
      "Epoch: [40][20/36]\tTime  1.003 ( 0.728)\tData  0.846 ( 0.541)\tLoss 6.6588e+00 (6.7049e+00)\tAcc@1  26.95 ( 25.06)\tAcc@5  71.48 ( 71.26)\n",
      "Epoch: [40][30/36]\tTime  0.153 ( 0.600)\tData  0.000 ( 0.424)\tLoss 3.6418e+00 (6.2350e+00)\tAcc@1  26.95 ( 25.82)\tAcc@5  82.81 ( 71.74)\n",
      " * Acc@1 27.233 Acc@5 75.439\n",
      "EPOCH 41 ENDED: LOSS train 5.943762236566094 LOSS val 3.8175744567702643 Train Acc 26.216066360473633 Val Acc 27.2333927154541\n",
      "Epoch: [41][ 0/36]\tTime  7.530 ( 7.530)\tData  7.201 ( 7.201)\tLoss 3.1552e+00 (3.1552e+00)\tAcc@1  32.81 ( 32.81)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [41][10/36]\tTime  0.152 ( 0.986)\tData  0.000 ( 0.817)\tLoss 5.1806e+00 (3.9976e+00)\tAcc@1  29.30 ( 28.94)\tAcc@5  71.09 ( 75.57)\n",
      "Epoch: [41][20/36]\tTime  1.060 ( 0.713)\tData  0.891 ( 0.545)\tLoss 5.6776e+00 (4.4155e+00)\tAcc@1  24.22 ( 28.52)\tAcc@5  66.02 ( 73.68)\n",
      "Epoch: [41][30/36]\tTime  0.174 ( 0.588)\tData  0.000 ( 0.419)\tLoss 7.8942e+00 (4.9637e+00)\tAcc@1  21.48 ( 27.38)\tAcc@5  61.33 ( 72.27)\n",
      " * Acc@1 15.576 Acc@5 65.233\n",
      "EPOCH 42 ENDED: LOSS train 5.404425126918465 LOSS val 11.109913840431025 Train Acc 26.86980628967285 Val Acc 15.576481819152832\n",
      "Epoch: [42][ 0/36]\tTime  7.335 ( 7.335)\tData  7.017 ( 7.017)\tLoss 1.0889e+01 (1.0889e+01)\tAcc@1  16.02 ( 16.02)\tAcc@5  62.89 ( 62.89)\n",
      "Epoch: [42][10/36]\tTime  0.159 ( 0.941)\tData  0.000 ( 0.774)\tLoss 5.7697e+00 (8.2531e+00)\tAcc@1  26.56 ( 21.52)\tAcc@5  76.56 ( 68.82)\n",
      "Epoch: [42][20/36]\tTime  1.051 ( 0.701)\tData  0.899 ( 0.541)\tLoss 8.4670e+00 (7.8003e+00)\tAcc@1  19.53 ( 22.67)\tAcc@5  71.88 ( 70.07)\n",
      "Epoch: [42][30/36]\tTime  0.150 ( 0.577)\tData  0.000 ( 0.419)\tLoss 6.6850e+00 (7.6899e+00)\tAcc@1  17.97 ( 22.93)\tAcc@5  72.66 ( 70.06)\n",
      " * Acc@1 22.270 Acc@5 69.611\n",
      "EPOCH 43 ENDED: LOSS train 7.703600570171494 LOSS val 6.643743673029914 Train Acc 22.72576141357422 Val Acc 22.27029800415039\n",
      "Epoch: [43][ 0/36]\tTime  7.503 ( 7.503)\tData  6.997 ( 6.997)\tLoss 7.4230e+00 (7.4230e+00)\tAcc@1  16.80 ( 16.80)\tAcc@5  64.06 ( 64.06)\n",
      "Epoch: [43][10/36]\tTime  0.153 ( 0.937)\tData  0.000 ( 0.752)\tLoss 6.9231e+00 (6.2979e+00)\tAcc@1  21.48 ( 24.22)\tAcc@5  71.09 ( 71.98)\n",
      "Epoch: [43][20/36]\tTime  0.988 ( 0.689)\tData  0.816 ( 0.517)\tLoss 5.0672e+00 (5.7374e+00)\tAcc@1  27.34 ( 25.89)\tAcc@5  77.73 ( 73.29)\n",
      "Epoch: [43][30/36]\tTime  0.175 ( 0.577)\tData  0.000 ( 0.404)\tLoss 6.4936e+00 (5.7224e+00)\tAcc@1  17.19 ( 25.08)\tAcc@5  69.53 ( 72.85)\n",
      " * Acc@1 25.325 Acc@5 71.214\n",
      "EPOCH 44 ENDED: LOSS train 5.7659413262153265 LOSS val 6.850028646723494 Train Acc 25.218835830688477 Val Acc 25.324508666992188\n",
      "Epoch: [44][ 0/36]\tTime  7.468 ( 7.468)\tData  7.297 ( 7.297)\tLoss 5.0962e+00 (5.0962e+00)\tAcc@1  26.95 ( 26.95)\tAcc@5  76.56 ( 76.56)\n",
      "Epoch: [44][10/36]\tTime  0.153 ( 0.980)\tData  0.000 ( 0.821)\tLoss 3.8707e+00 (4.9816e+00)\tAcc@1  31.25 ( 27.81)\tAcc@5  78.91 ( 73.65)\n",
      "Epoch: [44][20/36]\tTime  1.034 ( 0.716)\tData  0.883 ( 0.561)\tLoss 4.9427e+00 (4.7416e+00)\tAcc@1  26.56 ( 27.25)\tAcc@5  73.44 ( 74.59)\n",
      "Epoch: [44][30/36]\tTime  0.151 ( 0.594)\tData  0.000 ( 0.439)\tLoss 3.6909e+00 (4.4731e+00)\tAcc@1  31.64 ( 28.07)\tAcc@5  71.88 ( 75.08)\n",
      " * Acc@1 21.405 Acc@5 67.371\n",
      "EPOCH 45 ENDED: LOSS train 4.393751489633999 LOSS val 5.069170687634941 Train Acc 28.110803604125977 Val Acc 21.404937744140625\n",
      "Epoch: [45][ 0/36]\tTime  7.606 ( 7.606)\tData  7.127 ( 7.127)\tLoss 4.4644e+00 (4.4644e+00)\tAcc@1  21.09 ( 21.09)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [45][10/36]\tTime  0.152 ( 0.994)\tData  0.001 ( 0.811)\tLoss 3.1207e+00 (3.7989e+00)\tAcc@1  29.30 ( 26.14)\tAcc@5  76.95 ( 75.71)\n",
      "Epoch: [45][20/36]\tTime  0.856 ( 0.709)\tData  0.705 ( 0.540)\tLoss 2.9195e+00 (3.4121e+00)\tAcc@1  34.77 ( 29.28)\tAcc@5  78.91 ( 77.99)\n",
      "Epoch: [45][30/36]\tTime  0.153 ( 0.582)\tData  0.000 ( 0.418)\tLoss 3.2798e+00 (3.3114e+00)\tAcc@1  31.64 ( 29.65)\tAcc@5  76.56 ( 77.91)\n",
      " * Acc@1 30.720 Acc@5 73.785\n",
      "EPOCH 46 ENDED: LOSS train 3.3737441323932846 LOSS val 3.8173020188970463 Train Acc 29.407201766967773 Val Acc 30.72028350830078\n",
      "Epoch: [46][ 0/36]\tTime  7.421 ( 7.421)\tData  7.257 ( 7.257)\tLoss 4.2071e+00 (4.2071e+00)\tAcc@1  29.69 ( 29.69)\tAcc@5  75.00 ( 75.00)\n",
      "Epoch: [46][10/36]\tTime  0.152 ( 0.984)\tData  0.000 ( 0.829)\tLoss 4.3171e+00 (4.4675e+00)\tAcc@1  26.56 ( 26.10)\tAcc@5  75.39 ( 73.93)\n",
      "Epoch: [46][20/36]\tTime  0.899 ( 0.709)\tData  0.747 ( 0.555)\tLoss 5.9551e+00 (4.7744e+00)\tAcc@1  25.78 ( 26.41)\tAcc@5  71.48 ( 73.53)\n",
      "Epoch: [46][30/36]\tTime  0.162 ( 0.591)\tData  0.000 ( 0.436)\tLoss 5.5715e+00 (4.8893e+00)\tAcc@1  25.39 ( 26.02)\tAcc@5  71.48 ( 73.31)\n",
      " * Acc@1 23.314 Acc@5 71.672\n",
      "EPOCH 47 ENDED: LOSS train 4.823800014062601 LOSS val 4.820862001060802 Train Acc 26.30470848083496 Val Acc 23.313819885253906\n",
      "Epoch: [47][ 0/36]\tTime  7.400 ( 7.400)\tData  7.055 ( 7.055)\tLoss 4.3995e+00 (4.3995e+00)\tAcc@1  26.95 ( 26.95)\tAcc@5  73.44 ( 73.44)\n",
      "Epoch: [47][10/36]\tTime  0.151 ( 0.953)\tData  0.000 ( 0.784)\tLoss 3.9009e+00 (4.4653e+00)\tAcc@1  26.95 ( 26.56)\tAcc@5  81.64 ( 75.75)\n",
      "Epoch: [47][20/36]\tTime  0.926 ( 0.690)\tData  0.774 ( 0.529)\tLoss 3.5518e+00 (4.2186e+00)\tAcc@1  27.73 ( 26.69)\tAcc@5  76.56 ( 75.17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47][30/36]\tTime  0.151 ( 0.575)\tData  0.000 ( 0.416)\tLoss 3.3969e+00 (3.9051e+00)\tAcc@1  27.73 ( 28.05)\tAcc@5  82.03 ( 76.45)\n",
      " * Acc@1 28.022 Acc@5 73.963\n",
      "EPOCH 48 ENDED: LOSS train 3.8475734048933203 LOSS val 4.189121214889576 Train Acc 28.365652084350586 Val Acc 28.022396087646484\n",
      "Epoch: [48][ 0/36]\tTime  7.496 ( 7.496)\tData  7.086 ( 7.086)\tLoss 4.2904e+00 (4.2904e+00)\tAcc@1  25.00 ( 25.00)\tAcc@5  73.05 ( 73.05)\n",
      "Epoch: [48][10/36]\tTime  0.496 ( 0.982)\tData  0.344 ( 0.807)\tLoss 3.5219e+00 (3.6583e+00)\tAcc@1  22.66 ( 27.34)\tAcc@5  75.78 ( 76.53)\n",
      "Epoch: [48][20/36]\tTime  0.714 ( 0.698)\tData  0.555 ( 0.533)\tLoss 3.0449e+00 (3.6040e+00)\tAcc@1  35.94 ( 27.86)\tAcc@5  84.77 ( 77.12)\n",
      "Epoch: [48][30/36]\tTime  0.441 ( 0.583)\tData  0.290 ( 0.423)\tLoss 3.4562e+00 (3.6040e+00)\tAcc@1  30.08 ( 27.95)\tAcc@5  72.66 ( 76.83)\n",
      " * Acc@1 27.081 Acc@5 73.607\n",
      "EPOCH 49 ENDED: LOSS train 3.549212274075875 LOSS val 3.9051431263333574 Train Acc 28.02216148376465 Val Acc 27.08068084716797\n",
      "Epoch: [49][ 0/36]\tTime  7.675 ( 7.675)\tData  7.265 ( 7.265)\tLoss 3.5838e+00 (3.5838e+00)\tAcc@1  24.61 ( 24.61)\tAcc@5  70.70 ( 70.70)\n",
      "Epoch: [49][10/36]\tTime  0.155 ( 0.979)\tData  0.000 ( 0.802)\tLoss 2.9311e+00 (3.2392e+00)\tAcc@1  35.16 ( 30.22)\tAcc@5  80.08 ( 78.44)\n",
      "Epoch: [49][20/36]\tTime  1.222 ( 0.729)\tData  1.049 ( 0.556)\tLoss 2.6632e+00 (3.1330e+00)\tAcc@1  30.86 ( 30.32)\tAcc@5  81.25 ( 78.83)\n",
      "Epoch: [49][30/36]\tTime  0.151 ( 0.609)\tData  0.000 ( 0.437)\tLoss 3.7746e+00 (3.1522e+00)\tAcc@1  26.17 ( 29.50)\tAcc@5  74.61 ( 78.28)\n",
      " * Acc@1 21.329 Acc@5 67.015\n",
      "EPOCH 50 ENDED: LOSS train 3.3947397909930537 LOSS val 6.236805365813176 Train Acc 28.65373992919922 Val Acc 21.328582763671875\n",
      "Epoch: [50][ 0/36]\tTime  8.193 ( 8.193)\tData  7.775 ( 7.775)\tLoss 5.3688e+00 (5.3688e+00)\tAcc@1  20.31 ( 20.31)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [50][10/36]\tTime  0.151 ( 1.009)\tData  0.001 ( 0.833)\tLoss 5.4145e+00 (5.6424e+00)\tAcc@1  23.83 ( 24.43)\tAcc@5  71.48 ( 69.32)\n",
      "Epoch: [50][20/36]\tTime  0.991 ( 0.719)\tData  0.840 ( 0.555)\tLoss 4.4397e+00 (5.1726e+00)\tAcc@1  29.30 ( 24.93)\tAcc@5  77.73 ( 70.67)\n",
      "Epoch: [50][30/36]\tTime  0.151 ( 0.586)\tData  0.000 ( 0.426)\tLoss 5.1259e+00 (4.7852e+00)\tAcc@1  33.98 ( 26.10)\tAcc@5  71.88 ( 72.29)\n",
      " * Acc@1 24.612 Acc@5 70.145\n",
      "EPOCH 51 ENDED: LOSS train 4.698262161032975 LOSS val 4.509148850820846 Train Acc 26.371191024780273 Val Acc 24.611860275268555\n",
      "Epoch: [51][ 0/36]\tTime  7.684 ( 7.684)\tData  7.166 ( 7.166)\tLoss 4.2149e+00 (4.2149e+00)\tAcc@1  25.78 ( 25.78)\tAcc@5  70.70 ( 70.70)\n",
      "Epoch: [51][10/36]\tTime  0.152 ( 0.958)\tData  0.000 ( 0.773)\tLoss 3.1404e+00 (3.8730e+00)\tAcc@1  32.42 ( 26.56)\tAcc@5  83.20 ( 74.36)\n",
      "Epoch: [51][20/36]\tTime  0.886 ( 0.693)\tData  0.735 ( 0.522)\tLoss 5.4932e+00 (3.8263e+00)\tAcc@1  26.56 ( 27.12)\tAcc@5  66.41 ( 75.11)\n",
      "Epoch: [51][30/36]\tTime  0.151 ( 0.574)\tData  0.000 ( 0.409)\tLoss 3.5762e+00 (4.0733e+00)\tAcc@1  29.69 ( 26.81)\tAcc@5  76.17 ( 74.45)\n",
      " * Acc@1 24.586 Acc@5 74.090\n",
      "EPOCH 52 ENDED: LOSS train 4.14005506993661 LOSS val 4.23654213580323 Train Acc 26.96953010559082 Val Acc 24.586408615112305\n",
      "Epoch: [52][ 0/36]\tTime  7.621 ( 7.621)\tData  7.310 ( 7.310)\tLoss 3.9644e+00 (3.9644e+00)\tAcc@1  24.22 ( 24.22)\tAcc@5  72.66 ( 72.66)\n",
      "Epoch: [52][10/36]\tTime  0.160 ( 0.972)\tData  0.000 ( 0.804)\tLoss 3.1674e+00 (3.4523e+00)\tAcc@1  31.25 ( 29.19)\tAcc@5  76.17 ( 75.64)\n",
      "Epoch: [52][20/36]\tTime  0.865 ( 0.693)\tData  0.715 ( 0.533)\tLoss 4.7333e+00 (3.4955e+00)\tAcc@1  20.31 ( 28.42)\tAcc@5  69.14 ( 75.69)\n",
      "Epoch: [52][30/36]\tTime  0.150 ( 0.581)\tData  0.000 ( 0.424)\tLoss 3.6440e+00 (3.5306e+00)\tAcc@1  29.69 ( 28.39)\tAcc@5  76.17 ( 76.20)\n",
      " * Acc@1 24.357 Acc@5 73.632\n",
      "EPOCH 53 ENDED: LOSS train 3.4984301732715806 LOSS val 3.78986794103888 Train Acc 28.686981201171875 Val Acc 24.357341766357422\n",
      "Epoch: [53][ 0/36]\tTime  7.760 ( 7.760)\tData  7.262 ( 7.262)\tLoss 3.2442e+00 (3.2442e+00)\tAcc@1  30.47 ( 30.47)\tAcc@5  77.34 ( 77.34)\n",
      "Epoch: [53][10/36]\tTime  0.152 ( 0.965)\tData  0.000 ( 0.780)\tLoss 2.7341e+00 (3.0738e+00)\tAcc@1  28.52 ( 31.75)\tAcc@5  80.86 ( 79.79)\n",
      "Epoch: [53][20/36]\tTime  1.000 ( 0.704)\tData  0.850 ( 0.535)\tLoss 3.4173e+00 (3.0753e+00)\tAcc@1  28.91 ( 30.78)\tAcc@5  76.56 ( 78.74)\n",
      "Epoch: [53][30/36]\tTime  0.152 ( 0.577)\tData  0.000 ( 0.413)\tLoss 3.2564e+00 (3.2391e+00)\tAcc@1  27.73 ( 29.85)\tAcc@5  72.27 ( 76.93)\n",
      " * Acc@1 29.906 Acc@5 75.566\n",
      "EPOCH 54 ENDED: LOSS train 3.2808400328205565 LOSS val 3.462339830204378 Train Acc 29.739612579345703 Val Acc 29.90582847595215\n",
      "Epoch: [54][ 0/36]\tTime  7.893 ( 7.893)\tData  7.469 ( 7.469)\tLoss 3.1281e+00 (3.1281e+00)\tAcc@1  35.16 ( 35.16)\tAcc@5  77.73 ( 77.73)\n",
      "Epoch: [54][10/36]\tTime  0.151 ( 0.966)\tData  0.000 ( 0.790)\tLoss 4.1944e+00 (3.3044e+00)\tAcc@1  24.61 ( 29.94)\tAcc@5  80.08 ( 78.66)\n",
      "Epoch: [54][20/36]\tTime  0.872 ( 0.693)\tData  0.721 ( 0.529)\tLoss 3.7877e+00 (3.4375e+00)\tAcc@1  23.44 ( 28.27)\tAcc@5  74.22 ( 76.53)\n",
      "Epoch: [54][30/36]\tTime  0.151 ( 0.573)\tData  0.000 ( 0.412)\tLoss 3.3027e+00 (3.3870e+00)\tAcc@1  29.69 ( 28.94)\tAcc@5  79.30 ( 76.54)\n",
      " * Acc@1 29.219 Acc@5 77.730\n",
      "EPOCH 55 ENDED: LOSS train 3.339994995613838 LOSS val 2.6248813378170692 Train Acc 29.650970458984375 Val Acc 29.218629837036133\n",
      "Epoch: [55][ 0/36]\tTime  7.653 ( 7.653)\tData  7.210 ( 7.210)\tLoss 2.2599e+00 (2.2599e+00)\tAcc@1  33.98 ( 33.98)\tAcc@5  78.91 ( 78.91)\n",
      "Epoch: [55][10/36]\tTime  0.153 ( 0.957)\tData  0.001 ( 0.777)\tLoss 2.5512e+00 (2.7474e+00)\tAcc@1  33.98 ( 32.81)\tAcc@5  80.08 ( 79.15)\n",
      "Epoch: [55][20/36]\tTime  0.902 ( 0.684)\tData  0.749 ( 0.517)\tLoss 2.7583e+00 (2.7018e+00)\tAcc@1  26.56 ( 32.46)\tAcc@5  78.91 ( 79.24)\n",
      "Epoch: [55][30/36]\tTime  0.155 ( 0.573)\tData  0.000 ( 0.410)\tLoss 2.5445e+00 (2.6498e+00)\tAcc@1  33.20 ( 32.42)\tAcc@5  81.25 ( 79.54)\n",
      " * Acc@1 24.256 Acc@5 77.679\n",
      "EPOCH 56 ENDED: LOSS train 2.65096287933413 LOSS val 3.236501753133015 Train Acc 32.232688903808594 Val Acc 24.255535125732422\n",
      "Epoch: [56][ 0/36]\tTime  7.836 ( 7.836)\tData  7.401 ( 7.401)\tLoss 3.1435e+00 (3.1435e+00)\tAcc@1  28.91 ( 28.91)\tAcc@5  83.59 ( 83.59)\n",
      "Epoch: [56][10/36]\tTime  0.172 ( 0.965)\tData  0.000 ( 0.784)\tLoss 2.9921e+00 (2.9357e+00)\tAcc@1  32.03 ( 31.61)\tAcc@5  74.22 ( 78.16)\n",
      "Epoch: [56][20/36]\tTime  1.002 ( 0.718)\tData  0.847 ( 0.550)\tLoss 2.7488e+00 (2.8991e+00)\tAcc@1  34.77 ( 31.49)\tAcc@5  76.95 ( 77.88)\n",
      "Epoch: [56][30/36]\tTime  0.153 ( 0.590)\tData  0.000 ( 0.427)\tLoss 2.3251e+00 (2.7487e+00)\tAcc@1  32.03 ( 32.04)\tAcc@5  78.12 ( 78.44)\n",
      " * Acc@1 33.444 Acc@5 80.631\n",
      "EPOCH 57 ENDED: LOSS train 2.700562299189475 LOSS val 2.499761238022899 Train Acc 32.40997314453125 Val Acc 33.44362258911133\n",
      "Epoch: [57][ 0/36]\tTime  8.117 ( 8.117)\tData  7.688 ( 7.688)\tLoss 2.3821e+00 (2.3821e+00)\tAcc@1  33.98 ( 33.98)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [57][10/36]\tTime  0.151 ( 1.013)\tData  0.000 ( 0.828)\tLoss 2.6188e+00 (2.3984e+00)\tAcc@1  31.64 ( 33.56)\tAcc@5  78.12 ( 81.00)\n",
      "Epoch: [57][20/36]\tTime  0.919 ( 0.724)\tData  0.769 ( 0.555)\tLoss 2.8234e+00 (2.5383e+00)\tAcc@1  24.61 ( 31.77)\tAcc@5  75.00 ( 79.45)\n",
      "Epoch: [57][30/36]\tTime  0.151 ( 0.596)\tData  0.000 ( 0.433)\tLoss 2.5497e+00 (2.5875e+00)\tAcc@1  33.98 ( 32.16)\tAcc@5  81.25 ( 79.62)\n",
      " * Acc@1 27.641 Acc@5 75.490\n",
      "EPOCH 58 ENDED: LOSS train 2.6322831178702146 LOSS val 3.095129755988137 Train Acc 31.734071731567383 Val Acc 27.640621185302734\n",
      "Epoch: [58][ 0/36]\tTime  7.669 ( 7.669)\tData  7.097 ( 7.097)\tLoss 2.6382e+00 (2.6382e+00)\tAcc@1  27.34 ( 27.34)\tAcc@5  78.52 ( 78.52)\n",
      "Epoch: [58][10/36]\tTime  0.151 ( 0.959)\tData  0.000 ( 0.768)\tLoss 2.6922e+00 (2.9940e+00)\tAcc@1  32.42 ( 30.29)\tAcc@5  76.95 ( 77.06)\n",
      "Epoch: [58][20/36]\tTime  0.802 ( 0.692)\tData  0.651 ( 0.520)\tLoss 2.6783e+00 (2.8692e+00)\tAcc@1  29.30 ( 30.90)\tAcc@5  78.91 ( 78.52)\n",
      "Epoch: [58][30/36]\tTime  0.151 ( 0.575)\tData  0.000 ( 0.410)\tLoss 2.0248e+00 (2.7307e+00)\tAcc@1  33.59 ( 30.62)\tAcc@5  83.59 ( 78.70)\n",
      " * Acc@1 27.233 Acc@5 72.054\n",
      "EPOCH 59 ENDED: LOSS train 2.6883653182138034 LOSS val 3.1355445953567207 Train Acc 31.069252014160156 Val Acc 27.2333927154541\n",
      "Epoch: [59][ 0/36]\tTime  7.393 ( 7.393)\tData  7.000 ( 7.000)\tLoss 2.8338e+00 (2.8338e+00)\tAcc@1  28.91 ( 28.91)\tAcc@5  75.39 ( 75.39)\n",
      "Epoch: [59][10/36]\tTime  0.151 ( 0.959)\tData  0.000 ( 0.786)\tLoss 2.6336e+00 (2.6479e+00)\tAcc@1  30.86 ( 30.82)\tAcc@5  76.56 ( 78.44)\n",
      "Epoch: [59][20/36]\tTime  1.225 ( 0.708)\tData  1.060 ( 0.545)\tLoss 2.4421e+00 (2.5679e+00)\tAcc@1  31.25 ( 31.47)\tAcc@5  82.03 ( 79.52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59][30/36]\tTime  0.152 ( 0.582)\tData  0.001 ( 0.422)\tLoss 2.7936e+00 (2.5499e+00)\tAcc@1  28.12 ( 31.51)\tAcc@5  79.69 ( 79.51)\n",
      " * Acc@1 30.797 Acc@5 79.893\n",
      "EPOCH 60 ENDED: LOSS train 2.5695287156303173 LOSS val 2.543824288583524 Train Acc 31.335180282592773 Val Acc 30.796640396118164\n",
      "Epoch: [60][ 0/36]\tTime  7.693 ( 7.693)\tData  7.371 ( 7.371)\tLoss 2.3448e+00 (2.3448e+00)\tAcc@1  37.11 ( 37.11)\tAcc@5  79.30 ( 79.30)\n",
      "Epoch: [60][10/36]\tTime  0.155 ( 1.010)\tData  0.000 ( 0.837)\tLoss 2.5415e+00 (2.3698e+00)\tAcc@1  24.61 ( 32.07)\tAcc@5  78.91 ( 79.76)\n",
      "Epoch: [60][20/36]\tTime  0.886 ( 0.721)\tData  0.733 ( 0.558)\tLoss 2.4575e+00 (2.3077e+00)\tAcc@1  26.17 ( 33.07)\tAcc@5  77.73 ( 80.99)\n",
      "Epoch: [60][30/36]\tTime  0.151 ( 0.604)\tData  0.000 ( 0.445)\tLoss 2.2634e+00 (2.2907e+00)\tAcc@1  33.20 ( 32.72)\tAcc@5  83.59 ( 80.91)\n",
      " * Acc@1 28.786 Acc@5 78.850\n",
      "EPOCH 61 ENDED: LOSS train 2.287807025473534 LOSS val 2.6924618650374748 Train Acc 32.86426544189453 Val Acc 28.78594970703125\n",
      "Epoch: [61][ 0/36]\tTime  7.739 ( 7.739)\tData  7.209 ( 7.209)\tLoss 2.4101e+00 (2.4101e+00)\tAcc@1  30.47 ( 30.47)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [61][10/36]\tTime  0.151 ( 0.940)\tData  0.000 ( 0.755)\tLoss 2.3006e+00 (2.3380e+00)\tAcc@1  32.03 ( 31.92)\tAcc@5  79.30 ( 80.97)\n",
      "Epoch: [61][20/36]\tTime  0.528 ( 0.669)\tData  0.378 ( 0.501)\tLoss 2.3893e+00 (2.3141e+00)\tAcc@1  30.08 ( 32.31)\tAcc@5  80.47 ( 80.49)\n",
      "Epoch: [61][30/36]\tTime  0.237 ( 0.562)\tData  0.086 ( 0.399)\tLoss 2.2905e+00 (2.3535e+00)\tAcc@1  32.81 ( 32.27)\tAcc@5  85.55 ( 80.37)\n",
      " * Acc@1 25.503 Acc@5 76.075\n",
      "EPOCH 62 ENDED: LOSS train 2.3651870978770164 LOSS val 2.8075898712783953 Train Acc 32.265926361083984 Val Acc 25.50267219543457\n",
      "Epoch: [62][ 0/36]\tTime  7.449 ( 7.449)\tData  7.276 ( 7.276)\tLoss 2.7266e+00 (2.7266e+00)\tAcc@1  30.86 ( 30.86)\tAcc@5  73.83 ( 73.83)\n",
      "Epoch: [62][10/36]\tTime  0.164 ( 1.003)\tData  0.001 ( 0.839)\tLoss 2.5275e+00 (2.4560e+00)\tAcc@1  29.69 ( 31.64)\tAcc@5  82.03 ( 80.01)\n",
      "Epoch: [62][20/36]\tTime  1.019 ( 0.718)\tData  0.867 ( 0.560)\tLoss 2.5586e+00 (2.3757e+00)\tAcc@1  28.91 ( 31.88)\tAcc@5  77.73 ( 80.17)\n",
      "Epoch: [62][30/36]\tTime  0.430 ( 0.604)\tData  0.280 ( 0.447)\tLoss 2.8011e+00 (2.3645e+00)\tAcc@1  30.08 ( 32.41)\tAcc@5  78.52 ( 80.07)\n",
      " * Acc@1 30.517 Acc@5 79.791\n",
      "EPOCH 63 ENDED: LOSS train 2.3502359219865454 LOSS val 2.3624158127370394 Train Acc 32.288089752197266 Val Acc 30.51667022705078\n",
      "Epoch: [63][ 0/36]\tTime  7.741 ( 7.741)\tData  7.329 ( 7.329)\tLoss 2.0523e+00 (2.0523e+00)\tAcc@1  34.77 ( 34.77)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [63][10/36]\tTime  0.200 ( 0.963)\tData  0.000 ( 0.781)\tLoss 2.2545e+00 (2.2361e+00)\tAcc@1  28.12 ( 35.12)\tAcc@5  80.86 ( 81.89)\n",
      "Epoch: [63][20/36]\tTime  1.182 ( 0.702)\tData  1.012 ( 0.532)\tLoss 2.0191e+00 (2.1903e+00)\tAcc@1  35.55 ( 34.78)\tAcc@5  81.64 ( 81.47)\n",
      "Epoch: [63][30/36]\tTime  0.158 ( 0.586)\tData  0.000 ( 0.419)\tLoss 2.0192e+00 (2.1892e+00)\tAcc@1  35.94 ( 34.19)\tAcc@5  83.20 ( 81.72)\n",
      " * Acc@1 30.262 Acc@5 79.384\n",
      "EPOCH 64 ENDED: LOSS train 2.1950969613886278 LOSS val 2.5277882524652013 Train Acc 33.82825469970703 Val Acc 30.26215171813965\n",
      "Epoch: [64][ 0/36]\tTime  7.245 ( 7.245)\tData  6.998 ( 6.998)\tLoss 2.3219e+00 (2.3219e+00)\tAcc@1  33.20 ( 33.20)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [64][10/36]\tTime  0.151 ( 0.957)\tData  0.000 ( 0.795)\tLoss 2.4584e+00 (2.3851e+00)\tAcc@1  37.11 ( 33.95)\tAcc@5  77.34 ( 79.83)\n",
      "Epoch: [64][20/36]\tTime  0.759 ( 0.697)\tData  0.607 ( 0.540)\tLoss 2.3268e+00 (2.3541e+00)\tAcc@1  34.38 ( 33.89)\tAcc@5  81.25 ( 80.13)\n",
      "Epoch: [64][30/36]\tTime  0.316 ( 0.578)\tData  0.166 ( 0.423)\tLoss 2.4791e+00 (2.3470e+00)\tAcc@1  33.20 ( 33.78)\tAcc@5  82.03 ( 80.34)\n",
      " * Acc@1 30.771 Acc@5 78.213\n",
      "EPOCH 65 ENDED: LOSS train 2.346485633017944 LOSS val 2.4000282626808613 Train Acc 33.86149597167969 Val Acc 30.771188735961914\n",
      "Epoch: [65][ 0/36]\tTime  7.678 ( 7.678)\tData  7.226 ( 7.226)\tLoss 2.2087e+00 (2.2087e+00)\tAcc@1  32.42 ( 32.42)\tAcc@5  78.91 ( 78.91)\n",
      "Epoch: [65][10/36]\tTime  0.151 ( 0.975)\tData  0.001 ( 0.794)\tLoss 1.9538e+00 (2.1979e+00)\tAcc@1  40.62 ( 35.01)\tAcc@5  87.11 ( 80.47)\n",
      "Epoch: [65][20/36]\tTime  1.143 ( 0.708)\tData  0.992 ( 0.540)\tLoss 2.1316e+00 (2.1834e+00)\tAcc@1  34.77 ( 34.62)\tAcc@5  78.52 ( 81.21)\n",
      "Epoch: [65][30/36]\tTime  0.152 ( 0.581)\tData  0.000 ( 0.418)\tLoss 2.1706e+00 (2.1982e+00)\tAcc@1  33.59 ( 34.16)\tAcc@5  82.03 ( 81.78)\n",
      " * Acc@1 31.866 Acc@5 79.002\n",
      "EPOCH 66 ENDED: LOSS train 2.2030214773320758 LOSS val 2.267677389747103 Train Acc 33.8836555480957 Val Acc 31.86561393737793\n",
      "Epoch: [66][ 0/36]\tTime  8.065 ( 8.065)\tData  7.651 ( 7.651)\tLoss 2.1803e+00 (2.1803e+00)\tAcc@1  32.81 ( 32.81)\tAcc@5  80.47 ( 80.47)\n",
      "Epoch: [66][10/36]\tTime  0.251 ( 0.999)\tData  0.100 ( 0.821)\tLoss 1.8684e+00 (2.1182e+00)\tAcc@1  35.16 ( 33.88)\tAcc@5  83.98 ( 81.61)\n",
      "Epoch: [66][20/36]\tTime  0.925 ( 0.714)\tData  0.775 ( 0.549)\tLoss 2.0822e+00 (2.1143e+00)\tAcc@1  33.59 ( 34.32)\tAcc@5  81.64 ( 81.90)\n",
      "Epoch: [66][30/36]\tTime  0.243 ( 0.587)\tData  0.083 ( 0.425)\tLoss 2.1349e+00 (2.1168e+00)\tAcc@1  32.81 ( 34.11)\tAcc@5  83.59 ( 82.14)\n",
      " * Acc@1 33.724 Acc@5 82.489\n",
      "EPOCH 67 ENDED: LOSS train 2.1126372285166606 LOSS val 2.18799098692768 Train Acc 34.13850402832031 Val Acc 33.723594665527344\n",
      "Epoch: [67][ 0/36]\tTime  7.263 ( 7.263)\tData  6.854 ( 6.854)\tLoss 1.9862e+00 (1.9862e+00)\tAcc@1  37.11 ( 37.11)\tAcc@5  83.59 ( 83.59)\n",
      "Epoch: [67][10/36]\tTime  0.333 ( 0.942)\tData  0.183 ( 0.763)\tLoss 1.8343e+00 (2.0411e+00)\tAcc@1  40.23 ( 36.58)\tAcc@5  84.38 ( 82.35)\n",
      "Epoch: [67][20/36]\tTime  0.807 ( 0.692)\tData  0.657 ( 0.523)\tLoss 1.9075e+00 (2.0349e+00)\tAcc@1  39.84 ( 36.33)\tAcc@5  83.98 ( 82.59)\n",
      "Epoch: [67][30/36]\tTime  0.407 ( 0.583)\tData  0.246 ( 0.419)\tLoss 1.9654e+00 (2.0589e+00)\tAcc@1  41.41 ( 35.77)\tAcc@5  84.38 ( 82.54)\n",
      " * Acc@1 29.829 Acc@5 78.951\n",
      "EPOCH 68 ENDED: LOSS train 2.0645254156926334 LOSS val 2.3369701718762195 Train Acc 35.77839279174805 Val Acc 29.829471588134766\n",
      "Epoch: [68][ 0/36]\tTime  7.823 ( 7.823)\tData  7.401 ( 7.401)\tLoss 2.2054e+00 (2.2054e+00)\tAcc@1  35.55 ( 35.55)\tAcc@5  80.86 ( 80.86)\n",
      "Epoch: [68][10/36]\tTime  0.150 ( 1.013)\tData  0.000 ( 0.835)\tLoss 2.0746e+00 (2.1497e+00)\tAcc@1  33.20 ( 33.38)\tAcc@5  80.47 ( 81.11)\n",
      "Epoch: [68][20/36]\tTime  1.219 ( 0.735)\tData  1.068 ( 0.570)\tLoss 2.1079e+00 (2.1209e+00)\tAcc@1  32.81 ( 33.93)\tAcc@5  80.08 ( 81.92)\n",
      "Epoch: [68][30/36]\tTime  0.158 ( 0.607)\tData  0.001 ( 0.444)\tLoss 2.1909e+00 (2.1021e+00)\tAcc@1  32.03 ( 34.36)\tAcc@5  81.64 ( 82.33)\n",
      " * Acc@1 33.673 Acc@5 81.980\n",
      "EPOCH 69 ENDED: LOSS train 2.084986802375878 LOSS val 2.0834464397282355 Train Acc 34.603878021240234 Val Acc 33.672691345214844\n",
      "Epoch: [69][ 0/36]\tTime  7.468 ( 7.468)\tData  7.101 ( 7.101)\tLoss 2.0982e+00 (2.0982e+00)\tAcc@1  33.20 ( 33.20)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [69][10/36]\tTime  0.156 ( 0.964)\tData  0.000 ( 0.791)\tLoss 1.9785e+00 (2.0657e+00)\tAcc@1  31.25 ( 34.09)\tAcc@5  83.59 ( 82.92)\n",
      "Epoch: [69][20/36]\tTime  0.832 ( 0.699)\tData  0.682 ( 0.531)\tLoss 2.0925e+00 (2.0412e+00)\tAcc@1  32.81 ( 34.47)\tAcc@5  80.86 ( 82.72)\n",
      "Epoch: [69][30/36]\tTime  0.154 ( 0.572)\tData  0.000 ( 0.409)\tLoss 1.8776e+00 (2.0088e+00)\tAcc@1  35.16 ( 35.28)\tAcc@5  84.77 ( 82.96)\n",
      " * Acc@1 32.146 Acc@5 81.751\n",
      "EPOCH 70 ENDED: LOSS train 2.0060138950717747 LOSS val 2.1497234243811407 Train Acc 35.3684196472168 Val Acc 32.14558410644531\n",
      "Epoch: [70][ 0/36]\tTime  7.441 ( 7.441)\tData  7.032 ( 7.032)\tLoss 2.0065e+00 (2.0065e+00)\tAcc@1  30.86 ( 30.86)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [70][10/36]\tTime  0.157 ( 0.963)\tData  0.000 ( 0.784)\tLoss 2.0330e+00 (2.0637e+00)\tAcc@1  38.28 ( 34.13)\tAcc@5  84.77 ( 83.88)\n",
      "Epoch: [70][20/36]\tTime  0.739 ( 0.693)\tData  0.588 ( 0.525)\tLoss 1.8311e+00 (2.0604e+00)\tAcc@1  40.23 ( 34.15)\tAcc@5  82.81 ( 82.74)\n",
      "Epoch: [70][30/36]\tTime  0.158 ( 0.583)\tData  0.000 ( 0.419)\tLoss 1.9745e+00 (2.0267e+00)\tAcc@1  36.72 ( 35.17)\tAcc@5  82.03 ( 83.28)\n",
      " * Acc@1 33.316 Acc@5 81.624\n",
      "EPOCH 71 ENDED: LOSS train 2.0161697735614723 LOSS val 2.11575253372673 Train Acc 35.324100494384766 Val Acc 33.31636428833008\n",
      "Epoch: [71][ 0/36]\tTime  7.574 ( 7.574)\tData  7.097 ( 7.097)\tLoss 2.1466e+00 (2.1466e+00)\tAcc@1  33.59 ( 33.59)\tAcc@5  79.30 ( 79.30)\n",
      "Epoch: [71][10/36]\tTime  0.159 ( 0.957)\tData  0.000 ( 0.774)\tLoss 1.8755e+00 (1.9434e+00)\tAcc@1  37.50 ( 37.25)\tAcc@5  83.20 ( 83.49)\n",
      "Epoch: [71][20/36]\tTime  0.886 ( 0.695)\tData  0.735 ( 0.526)\tLoss 1.7173e+00 (1.9491e+00)\tAcc@1  38.28 ( 36.63)\tAcc@5  88.28 ( 83.61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71][30/36]\tTime  0.151 ( 0.574)\tData  0.000 ( 0.410)\tLoss 1.8531e+00 (1.9449e+00)\tAcc@1  39.06 ( 36.77)\tAcc@5  84.38 ( 83.67)\n",
      " * Acc@1 34.004 Acc@5 81.598\n",
      "EPOCH 72 ENDED: LOSS train 1.9460276215888788 LOSS val 2.0486279622869477 Train Acc 36.66482162475586 Val Acc 34.003562927246094\n",
      "Epoch: [72][ 0/36]\tTime  7.465 ( 7.465)\tData  7.012 ( 7.012)\tLoss 1.9183e+00 (1.9183e+00)\tAcc@1  32.42 ( 32.42)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [72][10/36]\tTime  0.152 ( 0.973)\tData  0.001 ( 0.795)\tLoss 1.9272e+00 (1.9837e+00)\tAcc@1  38.28 ( 35.30)\tAcc@5  83.20 ( 82.32)\n",
      "Epoch: [72][20/36]\tTime  1.159 ( 0.717)\tData  1.005 ( 0.545)\tLoss 1.8610e+00 (1.9677e+00)\tAcc@1  37.50 ( 35.64)\tAcc@5  86.72 ( 83.05)\n",
      "Epoch: [72][30/36]\tTime  0.151 ( 0.599)\tData  0.000 ( 0.432)\tLoss 2.0266e+00 (1.9590e+00)\tAcc@1  29.69 ( 35.64)\tAcc@5  83.20 ( 83.33)\n",
      " * Acc@1 33.953 Acc@5 81.166\n",
      "EPOCH 73 ENDED: LOSS train 1.953398927413856 LOSS val 2.03623569491074 Train Acc 36.07756423950195 Val Acc 33.952659606933594\n",
      "Epoch: [73][ 0/36]\tTime  7.731 ( 7.731)\tData  7.268 ( 7.268)\tLoss 1.9206e+00 (1.9206e+00)\tAcc@1  33.98 ( 33.98)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [73][10/36]\tTime  0.181 ( 0.960)\tData  0.000 ( 0.777)\tLoss 2.0997e+00 (2.0409e+00)\tAcc@1  36.33 ( 35.90)\tAcc@5  82.03 ( 82.14)\n",
      "Epoch: [73][20/36]\tTime  0.929 ( 0.693)\tData  0.778 ( 0.522)\tLoss 2.0677e+00 (2.0008e+00)\tAcc@1  32.42 ( 36.18)\tAcc@5  78.52 ( 82.72)\n",
      "Epoch: [73][30/36]\tTime  0.153 ( 0.574)\tData  0.000 ( 0.410)\tLoss 2.0190e+00 (1.9873e+00)\tAcc@1  29.30 ( 36.16)\tAcc@5  84.38 ( 82.91)\n",
      " * Acc@1 34.156 Acc@5 81.191\n",
      "EPOCH 74 ENDED: LOSS train 1.9849071753124121 LOSS val 2.0582280831350324 Train Acc 36.21052551269531 Val Acc 34.156272888183594\n",
      "Epoch: [74][ 0/36]\tTime  7.579 ( 7.579)\tData  7.266 ( 7.266)\tLoss 1.7821e+00 (1.7821e+00)\tAcc@1  37.89 ( 37.89)\tAcc@5  88.28 ( 88.28)\n",
      "Epoch: [74][10/36]\tTime  0.151 ( 0.973)\tData  0.000 ( 0.802)\tLoss 1.9780e+00 (1.9865e+00)\tAcc@1  37.50 ( 35.40)\tAcc@5  79.30 ( 83.59)\n",
      "Epoch: [74][20/36]\tTime  1.086 ( 0.714)\tData  0.920 ( 0.551)\tLoss 1.9311e+00 (1.9591e+00)\tAcc@1  35.94 ( 36.09)\tAcc@5  85.16 ( 83.80)\n",
      "Epoch: [74][30/36]\tTime  0.154 ( 0.589)\tData  0.000 ( 0.427)\tLoss 1.8839e+00 (1.9472e+00)\tAcc@1  37.50 ( 36.27)\tAcc@5  84.77 ( 83.30)\n",
      " * Acc@1 34.029 Acc@5 81.700\n",
      "EPOCH 75 ENDED: LOSS train 1.9390497376846145 LOSS val 2.0700787485753285 Train Acc 36.64265823364258 Val Acc 34.029014587402344\n",
      "Epoch: [75][ 0/36]\tTime  7.375 ( 7.375)\tData  7.179 ( 7.179)\tLoss 2.0751e+00 (2.0751e+00)\tAcc@1  33.98 ( 33.98)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [75][10/36]\tTime  0.150 ( 0.967)\tData  0.000 ( 0.803)\tLoss 1.8309e+00 (1.9286e+00)\tAcc@1  39.45 ( 37.32)\tAcc@5  85.55 ( 83.03)\n",
      "Epoch: [75][20/36]\tTime  0.840 ( 0.695)\tData  0.689 ( 0.534)\tLoss 2.1042e+00 (1.9303e+00)\tAcc@1  29.69 ( 36.96)\tAcc@5  83.20 ( 83.44)\n",
      "Epoch: [75][30/36]\tTime  0.151 ( 0.580)\tData  0.000 ( 0.420)\tLoss 1.8672e+00 (1.9255e+00)\tAcc@1  39.06 ( 36.88)\tAcc@5  85.16 ( 83.61)\n",
      " * Acc@1 36.065 Acc@5 82.591\n",
      "EPOCH 76 ENDED: LOSS train 1.9189874721696172 LOSS val 1.9695731588950076 Train Acc 37.06371307373047 Val Acc 36.065155029296875\n",
      "Epoch: [76][ 0/36]\tTime  7.961 ( 7.961)\tData  7.416 ( 7.416)\tLoss 1.8725e+00 (1.8725e+00)\tAcc@1  40.23 ( 40.23)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [76][10/36]\tTime  0.151 ( 0.978)\tData  0.001 ( 0.787)\tLoss 1.8570e+00 (1.9583e+00)\tAcc@1  42.19 ( 36.19)\tAcc@5  86.33 ( 84.06)\n",
      "Epoch: [76][20/36]\tTime  0.976 ( 0.720)\tData  0.826 ( 0.544)\tLoss 1.9303e+00 (1.9275e+00)\tAcc@1  37.50 ( 35.81)\tAcc@5  80.47 ( 83.84)\n",
      "Epoch: [76][30/36]\tTime  0.151 ( 0.593)\tData  0.000 ( 0.424)\tLoss 1.8311e+00 (1.9161e+00)\tAcc@1  41.41 ( 36.28)\tAcc@5  82.03 ( 83.66)\n",
      " * Acc@1 35.480 Acc@5 82.744\n",
      "EPOCH 77 ENDED: LOSS train 1.9063124526042357 LOSS val 1.9764784310543073 Train Acc 36.47645568847656 Val Acc 35.479766845703125\n",
      "Epoch: [77][ 0/36]\tTime  7.411 ( 7.411)\tData  6.995 ( 6.995)\tLoss 1.8836e+00 (1.8836e+00)\tAcc@1  38.28 ( 38.28)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [77][10/36]\tTime  0.151 ( 0.943)\tData  0.000 ( 0.767)\tLoss 1.8939e+00 (1.8668e+00)\tAcc@1  41.02 ( 37.36)\tAcc@5  83.59 ( 83.74)\n",
      "Epoch: [77][20/36]\tTime  0.789 ( 0.661)\tData  0.622 ( 0.494)\tLoss 1.9705e+00 (1.8783e+00)\tAcc@1  35.16 ( 37.30)\tAcc@5  80.08 ( 83.89)\n",
      "Epoch: [77][30/36]\tTime  0.152 ( 0.563)\tData  0.000 ( 0.400)\tLoss 1.7257e+00 (1.8852e+00)\tAcc@1  42.58 ( 36.93)\tAcc@5  85.94 ( 84.06)\n",
      " * Acc@1 35.327 Acc@5 83.482\n",
      "EPOCH 78 ENDED: LOSS train 1.8914335857534013 LOSS val 1.9830631000997456 Train Acc 36.59833908081055 Val Acc 35.32705307006836\n",
      "Epoch: [78][ 0/36]\tTime  7.384 ( 7.384)\tData  6.966 ( 6.966)\tLoss 1.7145e+00 (1.7145e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  88.28 ( 88.28)\n",
      "Epoch: [78][10/36]\tTime  0.152 ( 0.927)\tData  0.001 ( 0.751)\tLoss 1.8789e+00 (1.8905e+00)\tAcc@1  35.94 ( 35.80)\tAcc@5  86.33 ( 84.16)\n",
      "Epoch: [78][20/36]\tTime  0.745 ( 0.676)\tData  0.585 ( 0.505)\tLoss 1.7598e+00 (1.8638e+00)\tAcc@1  39.84 ( 36.90)\tAcc@5  85.94 ( 84.19)\n",
      "Epoch: [78][30/36]\tTime  0.151 ( 0.564)\tData  0.000 ( 0.398)\tLoss 1.7564e+00 (1.8670e+00)\tAcc@1  39.45 ( 37.02)\tAcc@5  85.16 ( 84.19)\n",
      " * Acc@1 35.022 Acc@5 82.489\n",
      "EPOCH 79 ENDED: LOSS train 1.8688815966339323 LOSS val 1.9585050757103035 Train Acc 36.90858840942383 Val Acc 35.02163314819336\n",
      "Epoch: [79][ 0/36]\tTime  7.665 ( 7.665)\tData  7.260 ( 7.260)\tLoss 1.8809e+00 (1.8809e+00)\tAcc@1  38.67 ( 38.67)\tAcc@5  83.59 ( 83.59)\n",
      "Epoch: [79][10/36]\tTime  0.161 ( 0.953)\tData  0.010 ( 0.779)\tLoss 1.7929e+00 (1.8557e+00)\tAcc@1  36.72 ( 37.32)\tAcc@5  84.77 ( 84.45)\n",
      "Epoch: [79][20/36]\tTime  0.891 ( 0.694)\tData  0.740 ( 0.531)\tLoss 1.9321e+00 (1.8603e+00)\tAcc@1  38.28 ( 37.97)\tAcc@5  80.47 ( 84.36)\n",
      "Epoch: [79][30/36]\tTime  0.151 ( 0.570)\tData  0.000 ( 0.410)\tLoss 1.9058e+00 (1.8577e+00)\tAcc@1  35.16 ( 37.70)\tAcc@5  82.42 ( 84.35)\n",
      " * Acc@1 35.709 Acc@5 82.311\n",
      "EPOCH 80 ENDED: LOSS train 1.8588591473782823 LOSS val 1.9598292775298536 Train Acc 37.71745300292969 Val Acc 35.708831787109375\n",
      "Epoch: [80][ 0/36]\tTime  7.689 ( 7.689)\tData  7.282 ( 7.282)\tLoss 1.7064e+00 (1.7064e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  86.33 ( 86.33)\n",
      "Epoch: [80][10/36]\tTime  0.151 ( 0.951)\tData  0.000 ( 0.776)\tLoss 1.8469e+00 (1.8524e+00)\tAcc@1  38.28 ( 38.32)\tAcc@5  84.38 ( 84.45)\n",
      "Epoch: [80][20/36]\tTime  0.885 ( 0.680)\tData  0.734 ( 0.517)\tLoss 1.9810e+00 (1.8925e+00)\tAcc@1  30.08 ( 37.05)\tAcc@5  83.20 ( 83.39)\n",
      "Epoch: [80][30/36]\tTime  0.333 ( 0.564)\tData  0.177 ( 0.404)\tLoss 1.9296e+00 (1.8763e+00)\tAcc@1  37.50 ( 37.44)\tAcc@5  84.38 ( 83.66)\n",
      " * Acc@1 35.709 Acc@5 82.616\n",
      "EPOCH 81 ENDED: LOSS train 1.8789952723669543 LOSS val 1.9674607966200455 Train Acc 37.61772918701172 Val Acc 35.708831787109375\n",
      "Epoch: [81][ 0/36]\tTime  7.893 ( 7.893)\tData  7.451 ( 7.451)\tLoss 1.9083e+00 (1.9083e+00)\tAcc@1  37.50 ( 37.50)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [81][10/36]\tTime  0.163 ( 0.970)\tData  0.001 ( 0.788)\tLoss 1.8842e+00 (1.8542e+00)\tAcc@1  37.89 ( 38.71)\tAcc@5  81.25 ( 84.34)\n",
      "Epoch: [81][20/36]\tTime  0.490 ( 0.679)\tData  0.339 ( 0.511)\tLoss 1.6560e+00 (1.8648e+00)\tAcc@1  44.92 ( 37.63)\tAcc@5  87.50 ( 84.02)\n",
      "Epoch: [81][30/36]\tTime  0.151 ( 0.577)\tData  0.000 ( 0.415)\tLoss 1.8940e+00 (1.8386e+00)\tAcc@1  37.50 ( 38.27)\tAcc@5  85.55 ( 84.68)\n",
      " * Acc@1 36.091 Acc@5 82.769\n",
      "EPOCH 82 ENDED: LOSS train 1.8442327117127395 LOSS val 1.9326700016450384 Train Acc 38.049861907958984 Val Acc 36.090606689453125\n",
      "Epoch: [82][ 0/36]\tTime  7.843 ( 7.843)\tData  7.373 ( 7.373)\tLoss 1.9122e+00 (1.9122e+00)\tAcc@1  35.16 ( 35.16)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [82][10/36]\tTime  0.152 ( 0.991)\tData  0.001 ( 0.809)\tLoss 1.9921e+00 (1.8294e+00)\tAcc@1  36.72 ( 38.57)\tAcc@5  80.47 ( 84.16)\n",
      "Epoch: [82][20/36]\tTime  1.213 ( 0.723)\tData  1.062 ( 0.554)\tLoss 1.9156e+00 (1.8471e+00)\tAcc@1  34.77 ( 38.17)\tAcc@5  83.20 ( 83.89)\n",
      "Epoch: [82][30/36]\tTime  0.162 ( 0.595)\tData  0.000 ( 0.430)\tLoss 1.8865e+00 (1.8458e+00)\tAcc@1  32.81 ( 37.85)\tAcc@5  84.77 ( 84.22)\n",
      " * Acc@1 36.549 Acc@5 82.769\n",
      "EPOCH 83 ENDED: LOSS train 1.844407106793158 LOSS val 1.9461902701299467 Train Acc 38.02770233154297 Val Acc 36.54874038696289\n",
      "Epoch: [83][ 0/36]\tTime  7.902 ( 7.902)\tData  7.446 ( 7.446)\tLoss 1.9830e+00 (1.9830e+00)\tAcc@1  37.89 ( 37.89)\tAcc@5  77.34 ( 77.34)\n",
      "Epoch: [83][10/36]\tTime  0.151 ( 0.960)\tData  0.000 ( 0.780)\tLoss 1.9377e+00 (1.8408e+00)\tAcc@1  35.94 ( 39.06)\tAcc@5  83.98 ( 83.91)\n",
      "Epoch: [83][20/36]\tTime  1.048 ( 0.705)\tData  0.897 ( 0.539)\tLoss 1.8117e+00 (1.8463e+00)\tAcc@1  40.23 ( 38.30)\tAcc@5  85.16 ( 84.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83][30/36]\tTime  0.151 ( 0.585)\tData  0.000 ( 0.423)\tLoss 1.9335e+00 (1.8511e+00)\tAcc@1  36.33 ( 37.95)\tAcc@5  81.64 ( 83.97)\n",
      " * Acc@1 35.760 Acc@5 83.075\n",
      "EPOCH 84 ENDED: LOSS train 1.8568755827319918 LOSS val 1.9464833615359596 Train Acc 37.783935546875 Val Acc 35.759735107421875\n",
      "Epoch: [84][ 0/36]\tTime  7.622 ( 7.622)\tData  7.155 ( 7.155)\tLoss 1.7339e+00 (1.7339e+00)\tAcc@1  37.11 ( 37.11)\tAcc@5  86.72 ( 86.72)\n",
      "Epoch: [84][10/36]\tTime  0.151 ( 0.951)\tData  0.000 ( 0.771)\tLoss 1.8063e+00 (1.8280e+00)\tAcc@1  37.89 ( 38.64)\tAcc@5  83.20 ( 84.55)\n",
      "Epoch: [84][20/36]\tTime  0.737 ( 0.675)\tData  0.586 ( 0.508)\tLoss 1.7621e+00 (1.8382e+00)\tAcc@1  40.23 ( 38.39)\tAcc@5  83.20 ( 83.98)\n",
      "Epoch: [84][30/36]\tTime  0.151 ( 0.577)\tData  0.000 ( 0.416)\tLoss 1.9149e+00 (1.8326e+00)\tAcc@1  38.28 ( 38.61)\tAcc@5  83.98 ( 84.34)\n",
      " * Acc@1 35.607 Acc@5 83.075\n",
      "EPOCH 85 ENDED: LOSS train 1.8336381215468007 LOSS val 1.9366622191405107 Train Acc 38.614959716796875 Val Acc 35.607025146484375\n",
      "Epoch: [85][ 0/36]\tTime  7.745 ( 7.745)\tData  7.368 ( 7.368)\tLoss 1.7744e+00 (1.7744e+00)\tAcc@1  39.06 ( 39.06)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [85][10/36]\tTime  0.152 ( 0.968)\tData  0.000 ( 0.796)\tLoss 1.8387e+00 (1.8183e+00)\tAcc@1  39.06 ( 37.68)\tAcc@5  87.11 ( 85.09)\n",
      "Epoch: [85][20/36]\tTime  0.934 ( 0.695)\tData  0.783 ( 0.532)\tLoss 1.8085e+00 (1.8371e+00)\tAcc@1  37.50 ( 37.07)\tAcc@5  86.33 ( 84.84)\n",
      "Epoch: [85][30/36]\tTime  0.151 ( 0.570)\tData  0.000 ( 0.411)\tLoss 1.7943e+00 (1.8372e+00)\tAcc@1  38.28 ( 37.74)\tAcc@5  83.98 ( 84.82)\n",
      " * Acc@1 35.836 Acc@5 83.151\n",
      "EPOCH 86 ENDED: LOSS train 1.8276539297315222 LOSS val 1.9188048174800567 Train Acc 37.93906021118164 Val Acc 35.836090087890625\n",
      "Epoch: [86][ 0/36]\tTime  8.025 ( 8.025)\tData  7.529 ( 7.529)\tLoss 1.8527e+00 (1.8527e+00)\tAcc@1  35.94 ( 35.94)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [86][10/36]\tTime  0.153 ( 0.993)\tData  0.000 ( 0.807)\tLoss 1.9342e+00 (1.8482e+00)\tAcc@1  32.03 ( 37.39)\tAcc@5  81.25 ( 84.30)\n",
      "Epoch: [86][20/36]\tTime  1.026 ( 0.720)\tData  0.875 ( 0.551)\tLoss 1.7821e+00 (1.8255e+00)\tAcc@1  42.58 ( 38.06)\tAcc@5  81.64 ( 84.64)\n",
      "Epoch: [86][30/36]\tTime  0.152 ( 0.590)\tData  0.000 ( 0.426)\tLoss 1.8310e+00 (1.8263e+00)\tAcc@1  35.94 ( 37.99)\tAcc@5  82.03 ( 84.75)\n",
      " * Acc@1 35.938 Acc@5 82.998\n",
      "EPOCH 87 ENDED: LOSS train 1.8212160544646414 LOSS val 1.920881924813684 Train Acc 38.049861907958984 Val Acc 35.937896728515625\n",
      "Epoch: [87][ 0/36]\tTime  7.687 ( 7.687)\tData  7.352 ( 7.352)\tLoss 1.9717e+00 (1.9717e+00)\tAcc@1  33.20 ( 33.20)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [87][10/36]\tTime  0.151 ( 0.963)\tData  0.000 ( 0.785)\tLoss 1.8112e+00 (1.8250e+00)\tAcc@1  37.89 ( 39.24)\tAcc@5  82.81 ( 84.34)\n",
      "Epoch: [87][20/36]\tTime  1.101 ( 0.703)\tData  0.950 ( 0.538)\tLoss 1.8398e+00 (1.8176e+00)\tAcc@1  36.33 ( 38.67)\tAcc@5  84.38 ( 85.32)\n",
      "Epoch: [87][30/36]\tTime  0.153 ( 0.579)\tData  0.000 ( 0.418)\tLoss 1.7725e+00 (1.8377e+00)\tAcc@1  37.89 ( 38.48)\tAcc@5  87.89 ( 84.99)\n",
      " * Acc@1 35.658 Acc@5 82.896\n",
      "EPOCH 88 ENDED: LOSS train 1.8374574995701334 LOSS val 1.917077154019735 Train Acc 38.22714614868164 Val Acc 35.657928466796875\n",
      "Epoch: [88][ 0/36]\tTime  7.742 ( 7.742)\tData  7.281 ( 7.281)\tLoss 1.8615e+00 (1.8615e+00)\tAcc@1  36.72 ( 36.72)\tAcc@5  81.64 ( 81.64)\n",
      "Epoch: [88][10/36]\tTime  0.151 ( 0.962)\tData  0.000 ( 0.772)\tLoss 1.7650e+00 (1.8521e+00)\tAcc@1  39.84 ( 38.10)\tAcc@5  82.42 ( 84.34)\n",
      "Epoch: [88][20/36]\tTime  0.893 ( 0.688)\tData  0.742 ( 0.517)\tLoss 1.9321e+00 (1.8217e+00)\tAcc@1  34.38 ( 38.62)\tAcc@5  79.69 ( 84.56)\n",
      "Epoch: [88][30/36]\tTime  0.151 ( 0.568)\tData  0.000 ( 0.403)\tLoss 1.7368e+00 (1.8196e+00)\tAcc@1  39.45 ( 38.68)\tAcc@5  85.55 ( 84.44)\n",
      " * Acc@1 35.734 Acc@5 82.947\n",
      "EPOCH 89 ENDED: LOSS train 1.8123260485762704 LOSS val 1.917516979621427 Train Acc 38.69252014160156 Val Acc 35.734283447265625\n",
      "Epoch: [89][ 0/36]\tTime  7.520 ( 7.520)\tData  7.092 ( 7.092)\tLoss 1.7854e+00 (1.7854e+00)\tAcc@1  38.28 ( 38.28)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [89][10/36]\tTime  0.170 ( 0.949)\tData  0.000 ( 0.766)\tLoss 1.6713e+00 (1.7954e+00)\tAcc@1  40.23 ( 40.20)\tAcc@5  87.50 ( 84.69)\n",
      "Epoch: [89][20/36]\tTime  0.577 ( 0.671)\tData  0.425 ( 0.502)\tLoss 1.9434e+00 (1.8298e+00)\tAcc@1  34.77 ( 38.58)\tAcc@5  84.77 ( 84.17)\n",
      "Epoch: [89][30/36]\tTime  0.150 ( 0.570)\tData  0.000 ( 0.407)\tLoss 1.8057e+00 (1.8233e+00)\tAcc@1  39.06 ( 38.43)\tAcc@5  83.59 ( 84.38)\n",
      " * Acc@1 35.862 Acc@5 82.998\n",
      "EPOCH 90 ENDED: LOSS train 1.829160487803726 LOSS val 1.9166673805671783 Train Acc 38.09418487548828 Val Acc 35.861541748046875\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 12 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 12 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/franciscolm6/TFGInformatica/e/TFGIN-192\n"
     ]
    }
   ],
   "source": [
    "best_val_top1 = 0.\n",
    "best_val_top5 = 0.\n",
    "#Empezamos entrenamiento y validación\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, init_lr, epoch, epochs)\n",
    "    #Entrenamos\n",
    "    train_loss, train_top1, train_top5 = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    #Validamos\n",
    "    val_loss, val_top1, val_top5 = validate(val_loader, model, criterion)\n",
    "    #Guardamos el mejor modelo\n",
    "    if val_top1 > best_val_top1:\n",
    "        best_val_top1 = val_top1\n",
    "        name = 'best.pth'\n",
    "        torch.save(model, model_path + name)\n",
    "    if val_top5 > best_val_top5:\n",
    "        best_val_top5 = val_top5\n",
    "    print(f'EPOCH {epoch+1} ENDED: LOSS train {train_loss} LOSS val {val_loss} Train Acc {train_top1} Val Acc {val_top1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfe248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
